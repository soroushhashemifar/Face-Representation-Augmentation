{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XxH50o1ZKrhq"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install gdown --force-reinstall\n",
    "# !gdown --id 1TEOjzSBN6UZc1WQwb_huEhdFl1XyZel4\n",
    "# !unzip KDEF_CROPPED_ALIGNED.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VjuyB0EsHCqc"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !gdown --id 1Bd87admxOZvbIOAyTkGEntsEz3fyMt7H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install mlxtend\n",
    "# !pip install dlib\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_oDZvCVOTsG",
    "outputId": "837ada7e-7b92-428b-97fb-2069633e073e"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.image import extract_face_landmarks\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgLxc6ioKYRK",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# MagFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YtV_nfmxHGMq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from torchvision.models.utils import load_state_dict_from_url\n",
    "\n",
    "__all__ = ['iresnet18', 'iresnet34', 'iresnet50', 'iresnet100']\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class IBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1):\n",
    "        super(IBasicBlock, self).__init__()\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\n",
    "                'BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\n",
    "                \"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes, eps=2e-05, momentum=0.9)\n",
    "        self.conv1 = conv3x3(inplanes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, eps=2e-05, momentum=0.9)\n",
    "        self.prelu = nn.PReLU(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn3 = nn.BatchNorm2d(planes, eps=2e-05, momentum=0.9)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.prelu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class IResNet(nn.Module):\n",
    "    fc_scale = 7 * 7\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=512, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None):\n",
    "        super(IResNet, self).__init__()\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes, eps=2e-05, momentum=0.9)\n",
    "        self.prelu = nn.PReLU(self.inplanes)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=2)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(\n",
    "            512 * block.expansion, eps=2e-05, momentum=0.9)\n",
    "        self.dropout = nn.Dropout2d(p=0.4, inplace=True)\n",
    "        self.fc = nn.Linear(512 * block.expansion * self.fc_scale, num_classes)\n",
    "        self.features = nn.BatchNorm1d(num_classes, eps=2e-05, momentum=0.9)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, IBasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion,\n",
    "                               eps=2e-05, momentum=0.9),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.prelu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = self.features(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def _iresnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = IResNet(block, layers, **kwargs)\n",
    "    # if pretrained:\n",
    "    # state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "    #                                        progress=progress)\n",
    "    # model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "# def iresnet18(pretrained=False, progress=True, **kwargs):\n",
    "#     return _iresnet('iresnet18', IBasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
    "#                     **kwargs)\n",
    "\n",
    "\n",
    "# def iresnet34(pretrained=False, progress=True, **kwargs):\n",
    "#     return _iresnet('iresnet34', IBasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
    "#                     **kwargs)\n",
    "\n",
    "\n",
    "# def iresnet50(pretrained=False, progress=True, **kwargs):\n",
    "#     return _iresnet('iresnet50', IBasicBlock, [3, 4, 14, 3], pretrained, progress,\n",
    "#                     **kwargs)\n",
    "\n",
    "\n",
    "def iresnet100(pretrained=False, progress=True, **kwargs):\n",
    "    return _iresnet('iresnet100', IBasicBlock, [3, 13, 30, 3], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "w8PSbbFbHNC-"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import sys\n",
    "\n",
    "def clean_dict_inf(model, state_dict):\n",
    "    _state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        # # assert k[0:1] == 'features.module.'\n",
    "        # new_k = 'features.'+'.'.join(k.split('.')[2:])\n",
    "        new_k = '.'.join(k.split('.')[2:])\n",
    "        if new_k in model.state_dict().keys() and v.size() == model.state_dict()[new_k].size():\n",
    "            _state_dict[new_k] = v\n",
    "        \n",
    "        new_kk = '.'.join(k.split('.')[1:])\n",
    "        if new_kk in model.state_dict().keys() and v.size() == model.state_dict()[new_kk].size():\n",
    "            _state_dict[new_kk] = v\n",
    "            \n",
    "    num_model = len(model.state_dict().keys())\n",
    "    num_ckpt = len(_state_dict.keys())\n",
    "    if num_model != num_ckpt:\n",
    "        sys.exit(\"=> Not all weights loaded, model params: {}, loaded params: {}\".format(\n",
    "            num_model, num_ckpt))\n",
    "    return _state_dict\n",
    "    \n",
    "    \n",
    "def load_dict_inf(model, resume, cpu_mode=False):\n",
    "    if os.path.isfile(resume):\n",
    "        print(f'=> loading pth from {resume} ...')\n",
    "        if cpu_mode:\n",
    "            checkpoint = torch.load(resume, map_location=torch.device(\"cpu\"))\n",
    "        else:\n",
    "            checkpoint = torch.load(resume)\n",
    "        _state_dict = clean_dict_inf(model, checkpoint['state_dict'])\n",
    "        model_dict = model.state_dict()\n",
    "        model_dict.update(_state_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        # delete to release more space\n",
    "        del checkpoint\n",
    "        del _state_dict\n",
    "    else:\n",
    "        sys.exit(\"=> No checkpoint found at '{}'\".format(resume))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzOuZ6dCKbii"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /media/soroushh/Storage2/matrices\n",
    "# !mkdir -p /media/soroushh/Storage2/matrices/training\n",
    "# !mkdir -p /media/soroushh/Storage2/matrices/evaluation\n",
    "\n",
    "# magface = iresnet100(pretrained=False, num_classes=512)\n",
    "# magface = load_dict_inf(magface, \"./magface_epoch_00025.pth\")\n",
    "# magface = magface.to(\"cuda\")\n",
    "# magface.eval()\n",
    "\n",
    "# def get_landmarks(image_paths, image_size):\n",
    "#     database = {}\n",
    "#     for image_path in tqdm.tqdm(image_paths):\n",
    "#         filename = image_path.split(\"/\")[-1].split(\".\")[0]\n",
    "#         if not bool(re.match(r\"\\w{2}\\d{2}\\w{2}\\w+\", filename)):\n",
    "#             continue\n",
    "\n",
    "#         name = re.findall(r\"(\\w{2}\\d{2})\\w{2}\\w+\", filename)[0]\n",
    "#         pose_label = re.findall(r\"\\w{2}\\d{2}\\w{2}(\\w+)\", filename)[0]\n",
    "#         emotion_label = re.findall(r\"\\w{2}\\d{2}(\\w{2})\\w+\", filename)[0]\n",
    "\n",
    "#         # img = cv2.imread(image_path)\n",
    "#         img = cv2.imread(image_path.replace(\"CROPPED_ALIGNED\", \"KDEF\")) # get real images from KDEF for landmarks\n",
    "#         img = cv2.resize(img, (image_size, image_size))\n",
    "#         img = cv2.copyMakeBorder(img, 10, 10, 10, 10, cv2.BORDER_CONSTANT)\n",
    "#         pose_img = get_pose_image(img, image_size)\n",
    "\n",
    "#         if pose_img is None:\n",
    "#             continue\n",
    "\n",
    "#         person_db = database.get(name, [])\n",
    "#         person_db.append((emotion_label, pose_label, pose_img, image_path))\n",
    "#         database[name] = person_db\n",
    "        \n",
    "#     return database\n",
    "\n",
    "# def get_pose_image(img, image_size):\n",
    "#     landmarks = extract_face_landmarks(img)\n",
    "\n",
    "#     if np.all(landmarks == 0) or landmarks is None:\n",
    "#         return None\n",
    "\n",
    "#     landmarks[:, 0][landmarks[:, 0] >= image_size] = image_size - 1\n",
    "#     landmarks[:, 0][landmarks[:, 0] < 0] = 0\n",
    "#     landmarks[:, 1][landmarks[:, 1] >= image_size] = image_size - 1\n",
    "#     landmarks[:, 1][landmarks[:, 1] < 0] = 0\n",
    "\n",
    "#     # landmarks = landmarks[[30, 40, 46, 48, 56]]\n",
    "#     # print(landmarks.shape)\n",
    "\n",
    "#     pose_img = np.zeros((image_size, image_size))\n",
    "#     pose_img[landmarks[:, 1], landmarks[:, 0]] = 1\n",
    "#     pose_img = pose_img[:, :, np.newaxis]\n",
    "#     pose_img = pose_img[landmarks[:, 1].min():landmarks[:, 1].max(), landmarks[:, 0].min():landmarks[:, 0].max()]\n",
    "#     pose_img = cv2.resize(pose_img, (image_size, image_size))\n",
    "#     pose_img = np.where(pose_img > 0.5, 1, 0)\n",
    "\n",
    "#     # plt.imshow(pose_img)\n",
    "#     # plt.show()\n",
    "\n",
    "#     pose_img = pose_img[:, :, np.newaxis]\n",
    "\n",
    "#     return pose_img\n",
    "\n",
    "# def get_magface_embedding(image_path):\n",
    "#     img = cv2.imread(image_path)\n",
    "\n",
    "#     if img.shape[:2] != [112, 112]:\n",
    "#         img = cv2.resize(img, (112, 112))\n",
    "\n",
    "#     img = img[np.newaxis] / 255.\n",
    "#     img = torch.Tensor(img).to(\"cuda\").to(torch.float32)\n",
    "#     img = img.permute(0, 3, 1, 2)\n",
    "#     embedding = magface(img)\n",
    "#     embedding = embedding.detach().cpu().numpy()\n",
    "#     embedding = np.squeeze(embedding)\n",
    "\n",
    "#     return embedding\n",
    "\n",
    "# def prepare_final_database(database):\n",
    "#     db_keys = list(database.keys())\n",
    "\n",
    "#     final_database = []\n",
    "#     for person1_name in db_keys:\n",
    "#         person1 = database[person1_name]\n",
    "#         for triple_to_reconstruct_index, triple_to_reconstruct in enumerate(person1):\n",
    "#             pose_img_to_reconstruct = triple_to_reconstruct[2]\n",
    "\n",
    "#             available_poses_input = []\n",
    "#             available_poses_output = []\n",
    "#             for person2_name in db_keys:\n",
    "#                 person2 = database[person2_name]\n",
    "#                 available_poses_input = list(filter(lambda index: person2[index][0] == triple_to_reconstruct[0], range(len(person2))))\n",
    "#                 available_poses_output = list(filter(lambda index: person2[index][0] == triple_to_reconstruct[0] and person2[index][1] == triple_to_reconstruct[1], range(len(person2))))\n",
    "\n",
    "#                 for triple_to_change_input_index in available_poses_input:\n",
    "#                     for triple_to_change_output_index in available_poses_output:\n",
    "#                         final_database.append((person1_name, triple_to_reconstruct_index, person2_name, triple_to_change_input_index, triple_to_change_output_index))\n",
    "\n",
    "#                 if len(final_database) >= 30000:\n",
    "#                     break\n",
    "\n",
    "#             if len(final_database) >= 30000:\n",
    "#                 break\n",
    "\n",
    "#         if len(final_database) >= 30000:\n",
    "#             break\n",
    "            \n",
    "#     return final_database\n",
    "\n",
    "# def save_files(database, final_database, type_):\n",
    "#     for idx in tqdm.tqdm(range(len(final_database))):\n",
    "#         tup = final_database[idx]\n",
    "\n",
    "#         if os.path.isfile(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_pose_img_to_reconstruct.npy') and os.path.isfile(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_input.npy') and os.path.isfile(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_output.npy'):\n",
    "#             continue\n",
    "\n",
    "#         pose_img_to_reconstruct = database[tup[0]][tup[1]][2]\n",
    "#         image_path = database[tup[2]][tup[3]][3]\n",
    "#         embedding_input = get_magface_embedding(image_path)\n",
    "#         image_path = database[tup[2]][tup[4]][3]\n",
    "#         embedding_output = get_magface_embedding(image_path)\n",
    "\n",
    "#         with open(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_pose_img_to_reconstruct.npy', 'wb') as f:\n",
    "#             np.save(f, pose_img_to_reconstruct)\n",
    "\n",
    "#         with open(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_input.npy', 'wb') as f:\n",
    "#             np.save(f, embedding_input)\n",
    "\n",
    "#         with open(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_output.npy', 'wb') as f:\n",
    "#             np.save(f, embedding_output)\n",
    "\n",
    "#     with open(f'/media/soroushh/Storage2/database_{type_}.pickle', 'wb') as handle:\n",
    "#         pickle.dump(database, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# people_paths = glob.glob(os.path.join(\"./CROPPED_ALIGNED/*\"))\n",
    "# people_paths = np.array(people_paths)\n",
    "# np.random.shuffle(people_paths)\n",
    "# people_paths = people_paths.tolist()\n",
    "\n",
    "# index = int(0.8 * len(people_paths))\n",
    "# trainset = people_paths[:index]\n",
    "# evalset = people_paths[index:]\n",
    "\n",
    "# image_paths = [img_path for person_path in trainset for img_path in glob.glob(person_path + \"/*.JPG\")]\n",
    "# database = get_landmarks(image_paths, 112)\n",
    "# final_database = prepare_final_database(database)\n",
    "# save_files(database, final_database, \"training\")\n",
    "\n",
    "# image_paths = [img_path for person_path in evalset for img_path in glob.glob(person_path + \"/*.JPG\")]\n",
    "# database = get_landmarks(image_paths, 112)\n",
    "# final_database = prepare_final_database(database)\n",
    "# save_files(database, final_database, \"evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "i_zh8j5fUthG"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class CustomDataSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, type_=\"train\"):\n",
    "        self.type_ = type_\n",
    "        \n",
    "        all_files = list(set([re.findall(r\"(.+)_(\\d+)_(.+)_(\\d+)_(\\d+)_.+\", filename)[0] for filename in os.listdir(f\"/media/soroushh/Storage2/matrices/training\")]))\n",
    "        if self.type_ == \"train\":\n",
    "            all_files = all_files[:int((2/3) * len(all_files))]\n",
    "        else:\n",
    "            all_files = all_files[int((2/3) * len(all_files)):]\n",
    "        \n",
    "        self.final_database = []\n",
    "        for person1_name, triple_to_reconstruct_index, person2_name, triple_to_change_input_index, triple_to_change_output_index in all_files:\n",
    "            triple_to_reconstruct_index = int(triple_to_reconstruct_index)\n",
    "            triple_to_change_input_index = int(triple_to_change_input_index)\n",
    "            triple_to_change_output_index = int(triple_to_change_output_index)\n",
    "            self.final_database.append((person1_name, triple_to_reconstruct_index, person2_name, triple_to_change_input_index, triple_to_change_output_index))\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/database_training.pickle', 'rb') as handle:\n",
    "            self.database = pickle.load(handle)\n",
    "        \n",
    "        ids = list(self.database.keys())\n",
    "        self.ids_dict = {id:idx for idx, id in enumerate(list(self.database.keys()))}\n",
    "        self.emotions_dict = {emo:idx for idx, emo in enumerate(np.unique([triplet[0] for id in ids for triplet in self.database[id]]).tolist())}\n",
    "        self.poses_dict = {pose:idx for idx, pose in enumerate(np.unique([triplet[1] for id in ids for triplet in self.database[id]]).tolist())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.final_database)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tup = self.final_database[idx]\n",
    "        \n",
    "        with open(f'/media/soroushh/Storage2/matrices/training/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_pose_img_to_reconstruct.npy', 'rb') as f:\n",
    "            pose_img_to_reconstruct = np.load(f)\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/matrices/training/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_input.npy', 'rb') as f:\n",
    "            embedding_input = np.load(f)\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/matrices/training/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_output.npy', 'rb') as f:\n",
    "            embedding_output = np.load(f)\n",
    "            \n",
    "        expected_pose_label = self.database[tup[2]][tup[4]][1]\n",
    "        input_id = tup[2]\n",
    "        input_emo = self.database[tup[2]][tup[3]][0]\n",
    "        \n",
    "        while True:\n",
    "            available_negative_choices = list(filter(lambda item: item[2] != input_id, self.final_database))\n",
    "            if len(available_negative_choices) == 0:\n",
    "                continue\n",
    "                \n",
    "            neg_tup = random.choice(available_negative_choices)\n",
    "            break\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/matrices/training/{neg_tup[0]}_{neg_tup[1]}_{neg_tup[2]}_{neg_tup[3]}_{neg_tup[4]}_embedding_input.npy', 'rb') as f:\n",
    "            negative_embedding_input = np.load(f)\n",
    "            \n",
    "        return pose_img_to_reconstruct, embedding_input, embedding_output, negative_embedding_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImVkf7AKuSUN",
    "outputId": "828bf540-aeaf-41e0-84ea-f46e18bc8e0b"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataSet(type_=\"train\")\n",
    "val_dataset = CustomDataSet(type_=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIJhvo9U-Hkk",
    "outputId": "d9201fc0-3d0f-44c4-f4c4-83a7450bc082"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001, 10001)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0Lg-pwKy5UO-"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7T0Rr0yYSYtG"
   },
   "source": [
    "# Network Arch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
    "    return x\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        # out = cls\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2alxw4ZtNmIb"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class Reshape(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, *args):\n",
    "#         super(Reshape, self).__init__()\n",
    "#         self.shape = args\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x.view(self.shape)\n",
    "\n",
    "    \n",
    "# def scaled_dot_product(q, k, v, mask=None):\n",
    "#     d_k = q.size()[-1]\n",
    "#     attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "#     attn_logits = attn_logits / math.sqrt(d_k)\n",
    "#     if mask is not None:\n",
    "#         attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "        \n",
    "#     attention = F.softmax(attn_logits, dim=-1)\n",
    "#     values = torch.matmul(attention, v)\n",
    "    \n",
    "#     return values, attention\n",
    "\n",
    "\n",
    "# class MultiheadAttention(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim, embed_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = embed_dim // num_heads\n",
    "\n",
    "#         # Stack all weight matrices 1...h together for efficiency\n",
    "#         # Note that in many implementations you see \"bias=False\" which is optional\n",
    "#         self.qkv_proj = torch.nn.Linear(input_dim, 3*embed_dim)\n",
    "#         self.o_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "#         self._reset_parameters()\n",
    "\n",
    "#     def _reset_parameters(self):\n",
    "#         # Original Transformer initialization, see PyTorch documentation\n",
    "#         torch.nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "#         self.qkv_proj.bias.data.fill_(0)\n",
    "#         torch.nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "#         self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x, mask=None, return_attention=False):\n",
    "#         batch_size, seq_length, embed_dim = x.size()\n",
    "#         qkv = self.qkv_proj(x)\n",
    "\n",
    "#         # Separate Q, K, V from linear output\n",
    "#         qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "#         qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "#         q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "#         # Determine value outputs\n",
    "#         values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "#         values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "#         values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "#         o = self.o_proj(values)\n",
    "\n",
    "#         if return_attention:\n",
    "#             return o, attention\n",
    "#         else:\n",
    "#             return o\n",
    "\n",
    "\n",
    "class EmbeddingGeneratorDecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EmbeddingGeneratorDecoder, self).__init__()\n",
    "\n",
    "        # self.downsample1 = torch.nn.Conv1d(2, 8, 3, stride=1, padding=1)\n",
    "        # self.downsample2 = torch.nn.Conv1d(8, 64, 3, stride=1, padding=1)\n",
    "        \n",
    "        # self.multihead_attention = MultiheadAttention(input_dim=128, embed_dim=128, num_heads=4)\n",
    "        \n",
    "        self.vit = VisionTransformer(**{\n",
    "                                        'embed_dim': 128,\n",
    "                                        'hidden_dim': 256,\n",
    "                                        'num_heads': 4,\n",
    "                                        'num_layers': 4,\n",
    "                                        'patch_size': 8,\n",
    "                                        'num_channels': 1,\n",
    "                                        'num_patches': 32,\n",
    "                                        'num_classes': 512,\n",
    "                                        'dropout': 0.4\n",
    "                                    })\n",
    "        \n",
    "        self.normalizer = nn.LayerNorm(1024)\n",
    "        \n",
    "#         self.upsample1 = torch.nn.Conv1d(64, 8, 3, stride=1, padding=1)\n",
    "#         self.upsample2 = torch.nn.Conv1d(8, 1, 3, stride=1, padding=1)\n",
    "        \n",
    "#         self.upsample_block = torch.nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        # self.batchnorm1 = torch.nn.BatchNorm1d(1)\n",
    "        # self.batchnorm8 = torch.nn.BatchNorm1d(8)\n",
    "        # self.batchnorm64 = torch.nn.BatchNorm1d(64)\n",
    "        \n",
    "        # self.features = torch.nn.Sequential(\n",
    "        #     torch.nn.Linear(512, 512), \n",
    "        #     torch.nn.BatchNorm1d(512),\n",
    "        # )\n",
    "        \n",
    "#     def feature_extraction_downsample(self, x): # n, 2, 512\n",
    "#         skip_connections = []\n",
    "        \n",
    "#         x = self.downsample1(x) # n, 8, 512\n",
    "#         # x = self.batchnorm8(x)\n",
    "#         x = F.dropout(x, p=0.4)\n",
    "#         x = F.relu(x)\n",
    "#         skip_connections.append(x)\n",
    "#         x = F.max_pool1d(x, kernel_size=2) # n, 8, 256 \n",
    "#         # x = F.avg_pool1d(x, kernel_size=2) # n, 8, 256 \n",
    "        \n",
    "#         x = self.downsample2(x) # n, 64, 256\n",
    "#         # x = self.batchnorm64(x)\n",
    "#         x = F.dropout(x, p=0.4)\n",
    "#         x = F.relu(x)\n",
    "#         skip_connections.append(x)\n",
    "#         x = F.max_pool1d(x, kernel_size=2) # n, 64, 128\n",
    "#         # x = F.avg_pool1d(x, kernel_size=2) # n, 8, 256\n",
    "        \n",
    "#         return x, skip_connections\n",
    "    \n",
    "#     def feature_extraction_upsample(self, x, skip_connections): # n, 64, 128\n",
    "#         skip_connections = list(reversed(skip_connections))\n",
    "        \n",
    "#         x = self.upsample_block(x) # n, 64, 256\n",
    "#         x = x + skip_connections[0]\n",
    "#         x = self.upsample1(x) # n, 8, 256\n",
    "#         # x = self.batchnorm8(x)\n",
    "#         x = F.dropout(x, p=0.4)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "#         x = self.upsample_block(x) # n, 8, 512\n",
    "#         x = x + skip_connections[1]\n",
    "#         x = self.upsample2(x) # n, 1, 512\n",
    "#         # x = self.batchnorm1(x)\n",
    "#         x = F.dropout(x, p=0.4)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "    def forward(self, emb1, emb2):\n",
    "#         emb1 = emb1.unsqueeze(1)\n",
    "#         emb2 = emb2.unsqueeze(1)\n",
    "        \n",
    "#         comb = torch.cat([emb1, emb2], 1)\n",
    "#         # comb = comb.unsqueeze(1)\n",
    "#         comb, skip_connections = self.feature_extraction_downsample(comb)\n",
    "#         # att_comb = self.multihead_attention(comb)\n",
    "#         att_comb = comb\n",
    "#         att_comb = self.feature_extraction_upsample(att_comb, skip_connections)\n",
    "#         att_comb = att_comb.squeeze(1)\n",
    "#         embedding = self.features(att_comb)\n",
    "        \n",
    "        comb = torch.cat([emb1, emb2], 1)\n",
    "        comb = self.normalizer(comb)\n",
    "        comb = torch.reshape(comb, (-1, 32, 32))\n",
    "        comb = comb.unsqueeze(1)\n",
    "        embedding = self.vit(comb)\n",
    "\n",
    "        # comb = comb.unsqueeze(1)\n",
    "        # comb, skip_connections = self.feature_extraction_downsample(comb)\n",
    "        # # att_comb = self.multihead_attention(comb)\n",
    "        # att_comb = comb\n",
    "        # att_comb = self.feature_extraction_upsample(att_comb, skip_connections)\n",
    "        # att_comb = att_comb.squeeze(1)\n",
    "        \n",
    "        # embedding = self.features(att_comb)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "\n",
    "class ConvAutoencoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        self.decoder_emb_generator = EmbeddingGeneratorDecoder()\n",
    "        \n",
    "        self.encoder_layer1 = torch.nn.Conv2d(1, 4, 3, stride=1, padding=1)\n",
    "        self.encoder_layer2 = torch.nn.Conv2d(4, 16, 3, stride=1, padding=1)\n",
    "        self.encoder_layer3 = torch.nn.Conv2d(16, 64, 3, stride=1, padding=1)\n",
    "        self.encoder_layer4 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.encoder_layer5 = torch.nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.encoder_layer6 = torch.nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "        \n",
    "        self.upsample2 = torch.nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.upsample3 = torch.nn.Upsample(scale_factor=3, mode='nearest')\n",
    "        self.upsample4 = torch.nn.Upsample(scale_factor=4, mode='nearest')\n",
    "        \n",
    "        self.decoder_layer1 = torch.nn.Conv2d(512, 256, 2, stride=3, padding=2)\n",
    "        self.decoder_layer2 = torch.nn.Conv2d(256, 128, 3, stride=1, padding=0)\n",
    "        self.decoder_layer3 = torch.nn.Conv2d(128, 64, 3, stride=1, padding=1)\n",
    "        self.decoder_layer4 = torch.nn.Conv2d(64, 16, 3, stride=1, padding=1)\n",
    "        self.decoder_layer5 = torch.nn.Conv2d(16, 4, 3, stride=1, padding=1)\n",
    "        self.decoder_layer6 = torch.nn.Conv2d(4, 1, 3, stride=1, padding=1)\n",
    "        \n",
    "        # self.bn8 = torch.nn.BatchNorm2d(8)\n",
    "        # self.bn64 = torch.nn.BatchNorm2d(64)\n",
    "        # self.bn256 = torch.nn.BatchNorm2d(256)\n",
    "        # self.bn512 = torch.nn.BatchNorm2d(512)\n",
    "        \n",
    "    def encoder(self, x): # 1, 112, 112\n",
    "        x = self.encoder_layer1(x) # 4, 112, 112\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn8(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 4, 56, 56\n",
    "        \n",
    "        x = self.encoder_layer2(x) # 16, 56, 56\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn64(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 16, 28, 28\n",
    "\n",
    "        x = self.encoder_layer3(x) # 64, 28, 28\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn256(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 64, 14, 14\n",
    "        \n",
    "        x = self.encoder_layer4(x) #128, 14, 14\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn512(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 128, 7, 7\n",
    "        \n",
    "        x = self.encoder_layer5(x) #256, 7, 7\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn512(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 256, 3, 3\n",
    "        \n",
    "        x = self.encoder_layer6(x) #512, 3, 3\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn512(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 512, 1, 1\n",
    "        \n",
    "        x = torch.reshape(x, (-1, 512))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def decoder_pose_reconstructor(self, x):\n",
    "        x = torch.reshape(x, (-1, 512, 1, 1))\n",
    "        \n",
    "        x = self.upsample4(x) # 512, 4, 4\n",
    "        x = self.decoder_layer1(x) # 256, 3, 3\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn256(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.upsample3(x) # 256, 9, 9\n",
    "        x = self.decoder_layer2(x) # 128, 7, 7\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn256(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.upsample2(x) # 128, 14, 14\n",
    "        x = self.decoder_layer3(x) # 64, 14, 14\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn256(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.upsample2(x) # 64, 28, 28\n",
    "        x = self.decoder_layer4(x) # 16, 28, 28\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn64(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.upsample2(x) # 16, 56, 56\n",
    "        x = self.decoder_layer5(x) # 4, 56, 56\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn8(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.upsample2(x) # 4, 112, 112\n",
    "        x = self.decoder_layer6(x) # 1, 112, 112\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(self, pose_img, magface_embedding):\n",
    "        coded = self.encoder(pose_img)\n",
    "        reconstructed_pose = self.decoder_pose_reconstructor(coded)\n",
    "        \n",
    "        # Feature fusion\n",
    "        generated_embedding = self.decoder_emb_generator(coded, magface_embedding)\n",
    "        \n",
    "        # fc_out = self.fc_head(generated_embedding)\n",
    "        \n",
    "#         id_predictions = self.id_head(fc_out)\n",
    "#         id_predictions = F.softmax(id_predictions, dim=1)\n",
    "        id_predictions = None\n",
    "        \n",
    "#         emo_predictions = self.emotion_head(fc_out)\n",
    "#         emo_predictions = F.softmax(emo_predictions, dim=1)\n",
    "        emo_predictions = None\n",
    "        \n",
    "        return reconstructed_pose, generated_embedding, (id_predictions, emo_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrYx7R4BSbwu"
   },
   "source": [
    "# Train phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HpAt2ZXJ-rvI"
   },
   "outputs": [],
   "source": [
    "net = ConvAutoencoder()\n",
    "net = net.to(\"cuda\")\n",
    "\n",
    "state_dict = torch.load(\"./augmentor.pt\")\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "criterion_bce = torch.nn.BCELoss(reduction='mean')\n",
    "criterion_mse = torch.nn.MSELoss()\n",
    "# criterion_huber = torch.nn.HuberLoss()\n",
    "# criterion_mae = torch.nn.L1Loss()\n",
    "# criterion_cross = torch.nn.CrossEntropyLoss()\n",
    "criterion_triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwLvM_7_NmEA",
    "outputId": "79e0e8dd-2934-4c26-bd44-ad0f67199daa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [34:47<00:00, 26.42s/it]\n",
      "100%|██████████| 40/40 [13:34<00:00, 20.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 300 Loss 0.7688 (0.2335, 0.5353) Val_Loss 0.1472 (0.1208, 0.0264) LR 0.001\n",
      "Model saved @ 2022-02-03 12:19:26.902091, Loss = 0.7688, Val_Loss = 0.1472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [25:46<00:00, 19.58s/it]\n",
      "100%|██████████| 40/40 [12:46<00:00, 19.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 300 Loss 0.1261 (0.1065, 0.0196) Val_Loss 0.1122 (0.0979, 0.0143) LR 0.001\n",
      "Model saved @ 2022-02-03 12:57:59.835312, Loss = 0.1261, Val_Loss = 0.1122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [26:13<00:00, 19.92s/it]\n",
      "100%|██████████| 40/40 [11:45<00:00, 17.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 300 Loss 0.1081 (0.0941, 0.0141) Val_Loss 0.1031 (0.091, 0.0121) LR 0.001\n",
      "Model saved @ 2022-02-03 13:35:58.807871, Loss = 0.1081, Val_Loss = 0.1031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [23:50<00:00, 18.11s/it]\n",
      "100%|██████████| 40/40 [10:55<00:00, 16.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 300 Loss 0.0981 (0.0883, 0.0097) Val_Loss 0.0955 (0.0863, 0.0092) LR 0.001\n",
      "Model saved @ 2022-02-03 14:10:44.981844, Loss = 0.0981, Val_Loss = 0.0955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [23:27<00:00, 17.82s/it]\n",
      "100%|██████████| 40/40 [10:53<00:00, 16.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 / 300 Loss 0.0925 (0.0846, 0.0079) Val_Loss 0.0927 (0.0831, 0.0096) LR 0.001\n",
      "Model saved @ 2022-02-03 14:45:06.556934, Loss = 0.0925, Val_Loss = 0.0927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:20<00:00, 16.97s/it]\n",
      "100%|██████████| 40/40 [10:43<00:00, 16.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 / 300 Loss 0.0911 (0.0819, 0.0092) Val_Loss 0.09 (0.0813, 0.0086) LR 0.001\n",
      "Model saved @ 2022-02-03 15:18:11.090157, Loss = 0.0911, Val_Loss = 0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:14<00:00, 16.89s/it]\n",
      "100%|██████████| 40/40 [10:05<00:00, 15.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / 300 Loss 0.0856 (0.0782, 0.0074) Val_Loss 0.0841 (0.0738, 0.0103) LR 0.001\n",
      "Model saved @ 2022-02-03 15:50:30.322123, Loss = 0.0856, Val_Loss = 0.0841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:16<00:00, 16.16s/it]\n",
      "100%|██████████| 40/40 [10:20<00:00, 15.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 / 300 Loss 0.0799 (0.0728, 0.0071) Val_Loss 0.0801 (0.0725, 0.0077) LR 0.001\n",
      "Model saved @ 2022-02-03 16:22:07.536163, Loss = 0.0799, Val_Loss = 0.0801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:24<00:00, 16.26s/it]\n",
      "100%|██████████| 40/40 [10:10<00:00, 15.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 / 300 Loss 0.0776 (0.0717, 0.006) Val_Loss 0.0799 (0.0714, 0.0086) LR 0.001\n",
      "Model saved @ 2022-02-03 16:53:43.024240, Loss = 0.0776, Val_Loss = 0.0799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:28<00:00, 16.31s/it]\n",
      "100%|██████████| 40/40 [10:09<00:00, 15.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 300 Loss 0.0783 (0.0709, 0.0073) Val_Loss 0.0792 (0.0707, 0.0085) LR 0.001\n",
      "Model saved @ 2022-02-03 17:25:20.513782, Loss = 0.0783, Val_Loss = 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:14<00:00, 16.90s/it]\n",
      "100%|██████████| 40/40 [10:10<00:00, 15.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 / 300 Loss 0.0774 (0.0702, 0.0072) Val_Loss 0.0761 (0.0702, 0.0058) LR 0.001\n",
      "Model saved @ 2022-02-03 17:57:46.179825, Loss = 0.0774, Val_Loss = 0.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:34<00:00, 16.38s/it]\n",
      "100%|██████████| 40/40 [10:01<00:00, 15.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 / 300 Loss 0.0768 (0.0697, 0.0071) Val_Loss 0.0743 (0.0695, 0.0048) LR 0.001\n",
      "Model saved @ 2022-02-03 18:29:21.978877, Loss = 0.0768, Val_Loss = 0.0743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:43<00:00, 16.50s/it]\n",
      "100%|██████████| 40/40 [10:32<00:00, 15.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 / 300 Loss 0.0762 (0.0691, 0.0071) Val_Loss 0.0759 (0.0691, 0.0068) LR 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:18<00:00, 16.94s/it]\n",
      "100%|██████████| 40/40 [10:20<00:00, 15.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 / 300 Loss 0.078 (0.0688, 0.0092) Val_Loss 0.0757 (0.0687, 0.007) LR 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:09<00:00, 16.83s/it]\n",
      "100%|██████████| 40/40 [10:28<00:00, 15.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 / 300 Loss 0.0741 (0.0681, 0.0059) Val_Loss 0.0763 (0.0687, 0.0076) LR 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:33<00:00, 17.13s/it]\n",
      "100%|██████████| 40/40 [10:29<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 / 300 Loss 0.0751 (0.0678, 0.0073) Val_Loss 0.0733 (0.0678, 0.0055) LR 0.001\n",
      "Model saved @ 2022-02-03 20:39:57.214447, Loss = 0.0751, Val_Loss = 0.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:38<00:00, 17.19s/it]\n",
      "100%|██████████| 40/40 [10:33<00:00, 15.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 / 300 Loss 0.0738 (0.0676, 0.0063) Val_Loss 0.0724 (0.0673, 0.0051) LR 0.001\n",
      "Model saved @ 2022-02-03 21:13:08.955905, Loss = 0.0738, Val_Loss = 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:08<00:00, 16.81s/it]\n",
      "100%|██████████| 40/40 [10:08<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 / 300 Loss 0.0725 (0.0671, 0.0054) Val_Loss 0.0753 (0.0672, 0.0081) LR 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:48<00:00, 16.56s/it]\n",
      "100%|██████████| 40/40 [10:31<00:00, 15.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 / 300 Loss 0.0727 (0.0669, 0.0058) Val_Loss 0.0743 (0.0668, 0.0075) LR 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:41<00:00, 17.23s/it]\n",
      "100%|██████████| 40/40 [10:37<00:00, 15.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 / 300 Loss 0.0725 (0.0664, 0.0061) Val_Loss 0.0723 (0.0662, 0.0061) LR 0.001\n",
      "Model saved @ 2022-02-03 22:51:04.389689, Loss = 0.0725, Val_Loss = 0.0723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [23:17<00:00, 17.70s/it]\n",
      "100%|██████████| 40/40 [11:52<00:00, 17.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 300 Loss 0.0745 (0.0662, 0.0083) Val_Loss 0.0721 (0.0662, 0.0059) LR 0.001\n",
      "Model saved @ 2022-02-03 23:26:15.266503, Loss = 0.0745, Val_Loss = 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [23:50<00:00, 18.11s/it]\n",
      "100%|██████████| 40/40 [11:08<00:00, 16.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 / 300 Loss 0.072 (0.066, 0.006) Val_Loss 0.0706 (0.0663, 0.0043) LR 0.001\n",
      "Model saved @ 2022-02-04 00:01:14.316333, Loss = 0.072, Val_Loss = 0.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [25:05<00:00, 19.06s/it]\n",
      "100%|██████████| 40/40 [11:36<00:00, 17.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 / 300 Loss 0.0714 (0.0658, 0.0056) Val_Loss 0.0731 (0.066, 0.0071) LR 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [23:28<00:00, 17.83s/it]\n",
      "100%|██████████| 40/40 [10:44<00:00, 16.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 / 300 Loss 0.0719 (0.0655, 0.0064) Val_Loss 0.0698 (0.0652, 0.0046) LR 0.001\n",
      "Model saved @ 2022-02-04 01:12:09.872223, Loss = 0.0719, Val_Loss = 0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [23:24<00:00, 17.78s/it]\n",
      "100%|██████████| 40/40 [11:17<00:00, 16.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 / 300 Loss 0.0704 (0.0653, 0.0051) Val_Loss 0.0729 (0.065, 0.0078) LR 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [23:24<00:00, 17.78s/it]\n",
      "100%|██████████| 40/40 [11:38<00:00, 17.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 / 300 Loss 0.0714 (0.065, 0.0064) Val_Loss 0.076 (0.0717, 0.0043) LR 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [25:04<00:00, 19.04s/it]\n",
      "100%|██████████| 40/40 [11:58<00:00, 17.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 / 300 Loss 0.0735 (0.0661, 0.0075) Val_Loss 0.0721 (0.0649, 0.0072) LR 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [24:00<00:00, 18.24s/it]\n",
      "100%|██████████| 40/40 [10:29<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 / 300 Loss 0.0711 (0.0644, 0.0067) Val_Loss 0.0692 (0.0644, 0.0048) LR 0.0005\n",
      "Model saved @ 2022-02-04 03:33:26.714579, Loss = 0.0711, Val_Loss = 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:54<00:00, 16.64s/it]\n",
      "100%|██████████| 40/40 [10:14<00:00, 15.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 / 300 Loss 0.0686 (0.0641, 0.0045) Val_Loss 0.0685 (0.0641, 0.0044) LR 0.0005\n",
      "Model saved @ 2022-02-04 04:05:36.410545, Loss = 0.0686, Val_Loss = 0.0685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:40<00:00, 16.46s/it]\n",
      "100%|██████████| 40/40 [10:27<00:00, 15.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 / 300 Loss 0.0695 (0.0639, 0.0056) Val_Loss 0.0687 (0.0639, 0.0048) LR 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:22<00:00, 16.99s/it]\n",
      "100%|██████████| 40/40 [10:10<00:00, 15.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 / 300 Loss 0.0684 (0.0638, 0.0046) Val_Loss 0.0696 (0.0637, 0.0059) LR 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:28<00:00, 16.31s/it]\n",
      "100%|██████████| 40/40 [10:11<00:00, 15.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 300 Loss 0.068 (0.0636, 0.0043) Val_Loss 0.0684 (0.0636, 0.0048) LR 0.0005\n",
      "Model saved @ 2022-02-04 05:41:57.964864, Loss = 0.068, Val_Loss = 0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:23<00:00, 16.25s/it]\n",
      "100%|██████████| 40/40 [10:05<00:00, 15.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 / 300 Loss 0.0685 (0.0634, 0.0051) Val_Loss 0.0678 (0.0635, 0.0044) LR 0.0005\n",
      "Model saved @ 2022-02-04 06:13:26.739934, Loss = 0.0685, Val_Loss = 0.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:05<00:00, 16.78s/it]\n",
      "100%|██████████| 40/40 [10:23<00:00, 15.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 / 300 Loss 0.0684 (0.0632, 0.0052) Val_Loss 0.0669 (0.0633, 0.0037) LR 0.0005\n",
      "Model saved @ 2022-02-04 06:45:55.669696, Loss = 0.0684, Val_Loss = 0.0669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:08<00:00, 16.81s/it]\n",
      "100%|██████████| 40/40 [10:16<00:00, 15.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 / 300 Loss 0.0682 (0.0631, 0.0051) Val_Loss 0.0693 (0.0631, 0.0061) LR 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:12<00:00, 16.87s/it]\n",
      "100%|██████████| 40/40 [10:21<00:00, 15.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 / 300 Loss 0.0672 (0.0627, 0.0045) Val_Loss 0.0683 (0.0626, 0.0057) LR 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:32<00:00, 17.13s/it]\n",
      "100%|██████████| 40/40 [10:29<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 / 300 Loss 0.0669 (0.0624, 0.0045) Val_Loss 0.0669 (0.0626, 0.0043) LR 0.00025\n",
      "Model saved @ 2022-02-04 08:23:56.361638, Loss = 0.0669, Val_Loss = 0.0669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:09<00:00, 16.83s/it]\n",
      "100%|██████████| 40/40 [10:29<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 / 300 Loss 0.0662 (0.0624, 0.0038) Val_Loss 0.067 (0.0625, 0.0045) LR 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:48<00:00, 16.56s/it]\n",
      "100%|██████████| 40/40 [10:05<00:00, 15.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 / 300 Loss 0.0666 (0.0623, 0.0043) Val_Loss 0.0662 (0.0625, 0.0037) LR 0.00025\n",
      "Model saved @ 2022-02-04 09:28:28.679849, Loss = 0.0666, Val_Loss = 0.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [21:56<00:00, 16.66s/it]\n",
      "100%|██████████| 40/40 [10:15<00:00, 15.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 / 300 Loss 0.0677 (0.0622, 0.0054) Val_Loss 0.0658 (0.0623, 0.0036) LR 0.00025\n",
      "Model saved @ 2022-02-04 10:00:40.411237, Loss = 0.0677, Val_Loss = 0.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [22:26<00:00, 17.04s/it]\n",
      "100%|██████████| 40/40 [10:29<00:00, 15.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 / 300 Loss 0.0662 (0.0621, 0.004) Val_Loss 0.0676 (0.0622, 0.0054) LR 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 14/79 [04:20<19:09, 17.69s/it]"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "historical_loss = []\n",
    "historical_val_loss = []\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # training\n",
    "    losses1 = []\n",
    "    losses2 = []\n",
    "    # losses3 = []\n",
    "    # losses4 = []\n",
    "    losses = []\n",
    "    for data in tqdm.tqdm(train_dataloader):\n",
    "        pose_img = data[0]\n",
    "        pose_img = pose_img.permute(0, 3, 1, 2)\n",
    "        pose_img = pose_img.to(\"cuda\").to(torch.float32)\n",
    "        \n",
    "        input_emb = data[1]\n",
    "        input_emb = input_emb.to(\"cuda\").to(torch.float32)\n",
    "        \n",
    "        output_emb = data[2]\n",
    "        output_emb = output_emb.to(\"cuda\").to(torch.float32)\n",
    "        \n",
    "        negative_embedding = data[3]\n",
    "        negative_embedding = negative_embedding.to(\"cuda\").to(torch.float32)\n",
    "        \n",
    "        # expected_pose_label, input_id, input_emo = data[3:]\n",
    "        # expected_pose_label = expected_pose_label.to(\"cuda\")\n",
    "        # input_id = input_id.to(\"cuda\")\n",
    "        # input_emo = input_emo.to(\"cuda\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstructed_pose, generated_emb, (id_predictions, emo_predictions) = net(pose_img, input_emb)\n",
    "        \n",
    "        loss1 = criterion_bce(reconstructed_pose, pose_img)\n",
    "        losses1.append(loss1.item())\n",
    "        \n",
    "        # loss2 = criterion_mse(generated_emb, output_emb)\n",
    "        loss2 = criterion_triplet_loss(generated_emb, output_emb, negative_embedding)\n",
    "        losses2.append(loss2.item())\n",
    "        \n",
    "#         loss3 = criterion_cross(id_predictions, input_id)\n",
    "#         losses3.append(loss3.item())\n",
    "        \n",
    "#         loss4 = criterion_cross(emo_predictions, input_emo)\n",
    "#         losses4.append(loss4.item())\n",
    "        \n",
    "        loss = loss1 + loss2 #+ loss3 + loss4\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        val_losses1 = []\n",
    "        val_losses2 = []\n",
    "        # val_losses3 = []\n",
    "        # val_losses4 = []\n",
    "        val_losses = []\n",
    "        for val_data in tqdm.tqdm(val_dataloader):\n",
    "            pose_img = val_data[0]\n",
    "            pose_img = pose_img.permute(0, 3, 1, 2)\n",
    "            pose_img = pose_img.to(\"cuda\").to(torch.float32)\n",
    "            \n",
    "            input_emb = val_data[1]\n",
    "            input_emb = input_emb.to(\"cuda\").to(torch.float32)\n",
    "            \n",
    "            output_emb = val_data[2]\n",
    "            output_emb = output_emb.to(\"cuda\").to(torch.float32)\n",
    "            \n",
    "            negative_embedding = val_data[3]\n",
    "            negative_embedding = negative_embedding.to(\"cuda\").to(torch.float32)\n",
    "            \n",
    "            # expected_pose_label, input_id, input_emo = val_data[3:]\n",
    "            # expected_pose_label = expected_pose_label.to(\"cuda\")\n",
    "            # input_id = input_id.to(\"cuda\")\n",
    "            # input_emo = input_emo.to(\"cuda\")\n",
    "            \n",
    "            reconstructed_pose, generated_emb, (id_predictions, emo_predictions) = net(pose_img, input_emb)\n",
    "            \n",
    "            val_loss1 = criterion_bce(reconstructed_pose, pose_img)\n",
    "            val_losses1.append(val_loss1.item())\n",
    "            \n",
    "            # val_loss2 = criterion_mse(generated_emb, output_emb)\n",
    "            val_loss2 = criterion_triplet_loss(generated_emb, output_emb, negative_embedding)\n",
    "            val_losses2.append(val_loss2.item())\n",
    "            \n",
    "#             val_loss3 = criterion_cross(id_predictions, input_id)\n",
    "#             val_losses3.append(val_loss3.item())\n",
    "            \n",
    "#             val_loss4 = criterion_cross(emo_predictions, input_emo)\n",
    "#             val_losses4.append(val_loss4.item())\n",
    "            \n",
    "            val_loss = val_loss1 + val_loss2 #+ val_loss3 + val_loss4\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(\"Epoch\", epoch+1, \"/\", NUM_EPOCHS, \n",
    "          \"Loss\", round(np.mean(losses), 4), f\"({round(np.mean(losses1), 4)}, {round(np.mean(losses2), 4)})\", \n",
    "          \"Val_Loss\", round(np.mean(val_losses), 4), f\"({round(np.mean(val_losses1), 4)}, {round(np.mean(val_losses2), 4)})\", \n",
    "          \"LR\", current_lr)\n",
    "    \n",
    "    # print(\"Loss\", np.mean(losses), f\"({np.mean(losses1)}, {np.mean(losses2)}, {np.mean(losses3)}, {np.mean(losses4)})\")\n",
    "    # print(\"Val_Loss\", np.mean(val_losses), f\"({np.mean(val_losses1)}, {np.mean(val_losses2)}, {np.mean(val_losses3)}, {np.mean(val_losses4)})\")\n",
    "    # historical_loss.append((np.mean(losses), np.mean(losses1), np.mean(losses2), np.mean(losses3), np.mean(losses4)))\n",
    "    # historical_val_loss.append((np.mean(val_losses), np.mean(val_losses1), np.mean(val_losses2), np.mean(val_losses3), np.mean(val_losses4)))\n",
    "    historical_loss.append((np.mean(losses), np.mean(losses1), np.mean(losses2)))\n",
    "    historical_val_loss.append((np.mean(val_losses), np.mean(val_losses1), np.mean(val_losses2)))\n",
    "    \n",
    "    if historical_val_loss[-1][0] < best_loss:\n",
    "        best_loss = historical_val_loss[-1][0]\n",
    "        torch.save(net.state_dict(), \"./augmentor.pt\")\n",
    "        print(f\"Model saved @ {datetime.datetime.now()}, Loss = {round(historical_loss[-1][0], 4)}, Val_Loss = {round(historical_val_loss[-1][0], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss\", round(loss.item(), 4), \"Val_Loss\", round(val_loss.item(), 4), \"LR\", current_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), \"./augmentor.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQS6umZYSeOT"
   },
   "source": [
    "# Evaluation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvAutoencoder()\n",
    "net = net.to(\"cuda\")\n",
    "state_dict = torch.load(\"./augmentor.pt\")\n",
    "net.load_state_dict(state_dict)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FGSkW8bXNmBo",
    "outputId": "99b6512a-7388-411c-e69e-2a2612400769"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in train_dataloader:\n",
    "        pose_img = data[0]\n",
    "        pose_img = pose_img.permute(0, 3, 1, 2)\n",
    "        pose_img = pose_img.to(\"cuda\").to(torch.float32)\n",
    "        input_emb = data[1]\n",
    "        input_emb = input_emb.to(\"cuda\").to(torch.float32)\n",
    "        output_emb = data[2]\n",
    "        output_emb = output_emb.to(\"cuda\").to(torch.float32)\n",
    "        \n",
    "        features = net.encoder(pose_img)\n",
    "        \n",
    "        reconstructed_data = net.decoder_pose_reconstructor(features)\n",
    "        \n",
    "        reconstructed_data = reconstructed_data.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        orig_data = pose_img.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        \n",
    "        for i in range(reconstructed_data.shape[0]):\n",
    "            plt.imshow(orig_data[i, :, :, 0])\n",
    "            plt.show()\n",
    "            plt.imshow(reconstructed_data[i, :, :, 0])\n",
    "            plt.show()\n",
    "            \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXwQU_ghCKBR"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in val_dataloader:\n",
    "        pose_img = data[0]\n",
    "        pose_img = pose_img.permute(0, 3, 1, 2)\n",
    "        pose_img = pose_img.to(\"cuda\").to(torch.float32)\n",
    "        input_emb = data[1]\n",
    "        input_emb = input_emb.to(\"cuda\").to(torch.float32)\n",
    "        output_emb = data[2]\n",
    "        output_emb = output_emb.to(\"cuda\").to(torch.float32)\n",
    "        \n",
    "        features, _ = net.encoder(pose_img)\n",
    "        \n",
    "        reconstructed_data = net.decoder_pose_reconstructor(features)\n",
    "        \n",
    "        reconstructed_data = reconstructed_data.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        orig_data = pose_img.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        \n",
    "        for i in range(reconstructed_data.shape[0]):\n",
    "            plt.imshow(orig_data[i, :, :, 0])\n",
    "            plt.show()\n",
    "            plt.imshow(reconstructed_data[i, :, :, 0])\n",
    "            plt.show()\n",
    "            \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mgLxc6ioKYRK"
   ],
   "name": "MeaningfulEmbeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
