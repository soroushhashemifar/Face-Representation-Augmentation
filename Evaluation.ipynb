{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9c0be6-2dbc-4ad6-9a36-76ba97814e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.image import extract_face_landmarks\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70deb9b8-c882-4f64-8f3a-1fa956a83c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class CustomDataSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, type_=\"train\"):\n",
    "        self.type_ = type_\n",
    "        \n",
    "        all_files = list(set([re.findall(r\"(.+)_(\\d+)_(.+)_(\\d+)_(\\d+)_.+\", filename)[0] for filename in os.listdir(f\"/media/soroushh/Storage2/matrices/evaluation\")]))\n",
    "        if self.type_ == \"train\":\n",
    "            all_files = all_files[:int((2/3) * len(all_files))]\n",
    "        else:\n",
    "            all_files = all_files[int((2/3) * len(all_files)):]\n",
    "        \n",
    "        self.final_database = []\n",
    "        for person1_name, triple_to_reconstruct_index, person2_name, triple_to_change_input_index, triple_to_change_output_index in all_files:\n",
    "            triple_to_reconstruct_index = int(triple_to_reconstruct_index)\n",
    "            triple_to_change_input_index = int(triple_to_change_input_index)\n",
    "            triple_to_change_output_index = int(triple_to_change_output_index)\n",
    "            self.final_database.append((person1_name, triple_to_reconstruct_index, person2_name, triple_to_change_input_index, triple_to_change_output_index))\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/database_evaluation.pickle', 'rb') as handle:\n",
    "            self.database = pickle.load(handle)\n",
    "        \n",
    "        ids = list(self.database.keys())\n",
    "        self.ids_dict = {id:idx for idx, id in enumerate(list(self.database.keys()))}\n",
    "        self.emotions_dict = {emo:idx for idx, emo in enumerate(np.unique([triplet[0] for id in ids for triplet in self.database[id]]).tolist())}\n",
    "        self.poses_dict = {pose:idx for idx, pose in enumerate(np.unique([triplet[1] for id in ids for triplet in self.database[id]]).tolist())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.final_database)\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        tup = self.final_database[idx]\n",
    "        \n",
    "        with open(f'/media/soroushh/Storage2/matrices/evaluation/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_pose_img_to_reconstruct.npy', 'rb') as f:\n",
    "            pose_img_to_reconstruct = np.load(f)\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/matrices/evaluation/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_input.npy', 'rb') as f:\n",
    "            embedding_input = np.load(f)\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/matrices/evaluation/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_output.npy', 'rb') as f:\n",
    "            embedding_output = np.load(f)\n",
    "            \n",
    "        expected_pose_label = self.database[tup[2]][tup[4]][1]\n",
    "        input_pose_label = self.database[tup[2]][tup[3]][1]\n",
    "        input_id = tup[2]\n",
    "        input_emo = self.database[tup[2]][tup[3]][0]\n",
    "        \n",
    "        while True:\n",
    "            available_negative_choices = list(filter(lambda item: item[2] != input_id, self.final_database))\n",
    "            if len(available_negative_choices) == 0:\n",
    "                continue\n",
    "                \n",
    "            neg_tup = random.choice(available_negative_choices)\n",
    "            break\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/matrices/evaluation/{neg_tup[0]}_{neg_tup[1]}_{neg_tup[2]}_{neg_tup[3]}_{neg_tup[4]}_embedding_input.npy', 'rb') as f:\n",
    "            negative_embedding_input = np.load(f)\n",
    "            \n",
    "        expected_pose_label = self.poses_dict[expected_pose_label]\n",
    "        input_pose_label = self.poses_dict[input_pose_label]\n",
    "        input_id = self.ids_dict[input_id]\n",
    "        input_emo = self.emotions_dict[input_emo]\n",
    "            \n",
    "        return pose_img_to_reconstruct, embedding_input, embedding_output, expected_pose_label, input_pose_label, input_id, input_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62111018-53a1-4653-9c41-c73661099638",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob.glob(os.path.join(\"./KDEF/*/*.JPG\"))\n",
    "image_paths = np.array(image_paths)\n",
    "np.random.shuffle(image_paths)\n",
    "image_paths = image_paths.tolist()\n",
    "\n",
    "index = int(0.9 * len(image_paths))\n",
    "# trainset = image_paths[:index]\n",
    "valset = image_paths[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad90db3-ada3-48e5-a071-3b26629f949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CustomDataSet(trainset, type_=\"train\", precomputerd=True)\n",
    "val_dataset = CustomDataSet(type_=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0e9b7c-a10c-46cf-8c98-30515adf84bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_dataset), \n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5bdef5-535a-45e2-903f-3f799a99ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
    "    return x\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        # out = cls\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a566c37-e349-4b70-af67-4c8baca7ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class Reshape(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, *args):\n",
    "#         super(Reshape, self).__init__()\n",
    "#         self.shape = args\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x.view(self.shape)\n",
    "\n",
    "    \n",
    "# def scaled_dot_product(q, k, v, mask=None):\n",
    "#     d_k = q.size()[-1]\n",
    "#     attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "#     attn_logits = attn_logits / math.sqrt(d_k)\n",
    "#     if mask is not None:\n",
    "#         attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "        \n",
    "#     attention = F.softmax(attn_logits, dim=-1)\n",
    "#     values = torch.matmul(attention, v)\n",
    "    \n",
    "#     return values, attention\n",
    "\n",
    "\n",
    "# class MultiheadAttention(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim, embed_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = embed_dim // num_heads\n",
    "\n",
    "#         # Stack all weight matrices 1...h together for efficiency\n",
    "#         # Note that in many implementations you see \"bias=False\" which is optional\n",
    "#         self.qkv_proj = torch.nn.Linear(input_dim, 3*embed_dim)\n",
    "#         self.o_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "#         self._reset_parameters()\n",
    "\n",
    "#     def _reset_parameters(self):\n",
    "#         # Original Transformer initialization, see PyTorch documentation\n",
    "#         torch.nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "#         self.qkv_proj.bias.data.fill_(0)\n",
    "#         torch.nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "#         self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "#     def forward(self, x, mask=None, return_attention=False):\n",
    "#         batch_size, seq_length, embed_dim = x.size()\n",
    "#         qkv = self.qkv_proj(x)\n",
    "\n",
    "#         # Separate Q, K, V from linear output\n",
    "#         qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "#         qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "#         q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "#         # Determine value outputs\n",
    "#         values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "#         values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "#         values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "#         o = self.o_proj(values)\n",
    "\n",
    "#         if return_attention:\n",
    "#             return o, attention\n",
    "#         else:\n",
    "#             return o\n",
    "\n",
    "\n",
    "class EmbeddingGeneratorDecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EmbeddingGeneratorDecoder, self).__init__()\n",
    "\n",
    "        # self.downsample1 = torch.nn.Conv1d(2, 8, 3, stride=1, padding=1)\n",
    "        # self.downsample2 = torch.nn.Conv1d(8, 64, 3, stride=1, padding=1)\n",
    "        \n",
    "        # self.multihead_attention = MultiheadAttention(input_dim=128, embed_dim=128, num_heads=4)\n",
    "        \n",
    "        self.vit = VisionTransformer(**{\n",
    "                                        'embed_dim': 128,\n",
    "                                        'hidden_dim': 256,\n",
    "                                        'num_heads': 4,\n",
    "                                        'num_layers': 4,\n",
    "                                        'patch_size': 8,\n",
    "                                        'num_channels': 1,\n",
    "                                        'num_patches': 32,\n",
    "                                        'num_classes': 512,\n",
    "                                        'dropout': 0.2\n",
    "                                    })\n",
    "        \n",
    "        self.normalizer = nn.LayerNorm(1024)\n",
    "        \n",
    "#         self.upsample1 = torch.nn.Conv1d(64, 8, 3, stride=1, padding=1)\n",
    "#         self.upsample2 = torch.nn.Conv1d(8, 1, 3, stride=1, padding=1)\n",
    "        \n",
    "#         self.upsample_block = torch.nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        # self.batchnorm1 = torch.nn.BatchNorm1d(1)\n",
    "        # self.batchnorm8 = torch.nn.BatchNorm1d(8)\n",
    "        # self.batchnorm64 = torch.nn.BatchNorm1d(64)\n",
    "        \n",
    "        # self.features = torch.nn.Sequential(\n",
    "        #     torch.nn.Linear(512, 512), \n",
    "        #     torch.nn.BatchNorm1d(512),\n",
    "        # )\n",
    "        \n",
    "#     def feature_extraction_downsample(self, x): # n, 2, 512\n",
    "#         skip_connections = []\n",
    "        \n",
    "#         x = self.downsample1(x) # n, 8, 512\n",
    "#         # x = self.batchnorm8(x)\n",
    "#         x = F.dropout(x, p=0.4)\n",
    "#         x = F.relu(x)\n",
    "#         skip_connections.append(x)\n",
    "#         x = F.max_pool1d(x, kernel_size=2) # n, 8, 256 \n",
    "#         # x = F.avg_pool1d(x, kernel_size=2) # n, 8, 256 \n",
    "        \n",
    "#         x = self.downsample2(x) # n, 64, 256\n",
    "#         # x = self.batchnorm64(x)\n",
    "#         x = F.dropout(x, p=0.4)\n",
    "#         x = F.relu(x)\n",
    "#         skip_connections.append(x)\n",
    "#         x = F.max_pool1d(x, kernel_size=2) # n, 64, 128\n",
    "#         # x = F.avg_pool1d(x, kernel_size=2) # n, 8, 256\n",
    "        \n",
    "#         return x, skip_connections\n",
    "    \n",
    "#     def feature_extraction_upsample(self, x, skip_connections): # n, 64, 128\n",
    "#         skip_connections = list(reversed(skip_connections))\n",
    "        \n",
    "#         x = self.upsample_block(x) # n, 64, 256\n",
    "#         x = x + skip_connections[0]\n",
    "#         x = self.upsample1(x) # n, 8, 256\n",
    "#         # x = self.batchnorm8(x)\n",
    "#         x = F.dropout(x, p=0.4)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "#         x = self.upsample_block(x) # n, 8, 512\n",
    "#         x = x + skip_connections[1]\n",
    "#         x = self.upsample2(x) # n, 1, 512\n",
    "#         # x = self.batchnorm1(x)\n",
    "#         x = F.dropout(x, p=0.4)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "    def forward(self, emb1, emb2):\n",
    "#         emb1 = emb1.unsqueeze(1)\n",
    "#         emb2 = emb2.unsqueeze(1)\n",
    "        \n",
    "#         comb = torch.cat([emb1, emb2], 1)\n",
    "#         # comb = comb.unsqueeze(1)\n",
    "#         comb, skip_connections = self.feature_extraction_downsample(comb)\n",
    "#         # att_comb = self.multihead_attention(comb)\n",
    "#         att_comb = comb\n",
    "#         att_comb = self.feature_extraction_upsample(att_comb, skip_connections)\n",
    "#         att_comb = att_comb.squeeze(1)\n",
    "#         embedding = self.features(att_comb)\n",
    "        \n",
    "        comb = torch.cat([emb1, emb2], 1)\n",
    "        comb = self.normalizer(comb)\n",
    "        comb = torch.reshape(comb, (-1, 32, 32))\n",
    "        comb = comb.unsqueeze(1)\n",
    "        embedding = self.vit(comb)\n",
    "\n",
    "        # comb = comb.unsqueeze(1)\n",
    "        # comb, skip_connections = self.feature_extraction_downsample(comb)\n",
    "        # # att_comb = self.multihead_attention(comb)\n",
    "        # att_comb = comb\n",
    "        # att_comb = self.feature_extraction_upsample(att_comb, skip_connections)\n",
    "        # att_comb = att_comb.squeeze(1)\n",
    "        \n",
    "        # embedding = self.features(att_comb)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "\n",
    "class ConvAutoencoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        self.decoder_emb_generator = EmbeddingGeneratorDecoder()\n",
    "        \n",
    "        self.encoder_layer1 = torch.nn.Conv2d(1, 4, 3, stride=1, padding=1)\n",
    "        self.encoder_layer2 = torch.nn.Conv2d(4, 16, 3, stride=1, padding=1)\n",
    "        self.encoder_layer3 = torch.nn.Conv2d(16, 64, 3, stride=1, padding=1)\n",
    "        self.encoder_layer4 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.encoder_layer5 = torch.nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.encoder_layer6 = torch.nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "        \n",
    "        self.upsample2 = torch.nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.upsample3 = torch.nn.Upsample(scale_factor=3, mode='nearest')\n",
    "        self.upsample4 = torch.nn.Upsample(scale_factor=4, mode='nearest')\n",
    "        \n",
    "        self.decoder_layer1 = torch.nn.Conv2d(512, 256, 2, stride=3, padding=2)\n",
    "        self.decoder_layer2 = torch.nn.Conv2d(256, 128, 3, stride=1, padding=0)\n",
    "        self.decoder_layer3 = torch.nn.Conv2d(128, 64, 3, stride=1, padding=1)\n",
    "        self.decoder_layer4 = torch.nn.Conv2d(64, 16, 3, stride=1, padding=1)\n",
    "        self.decoder_layer5 = torch.nn.Conv2d(16, 4, 3, stride=1, padding=1)\n",
    "        self.decoder_layer6 = torch.nn.Conv2d(4, 1, 3, stride=1, padding=1)\n",
    "        \n",
    "        # self.bn8 = torch.nn.BatchNorm2d(8)\n",
    "        # self.bn64 = torch.nn.BatchNorm2d(64)\n",
    "        # self.bn256 = torch.nn.BatchNorm2d(256)\n",
    "        # self.bn512 = torch.nn.BatchNorm2d(512)\n",
    "        \n",
    "    def encoder(self, x): # 1, 112, 112\n",
    "        x = self.encoder_layer1(x) # 4, 112, 112\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn8(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 4, 56, 56\n",
    "        \n",
    "        x = self.encoder_layer2(x) # 16, 56, 56\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn64(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 16, 28, 28\n",
    "\n",
    "        x = self.encoder_layer3(x) # 64, 28, 28\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn256(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 64, 14, 14\n",
    "        \n",
    "        x = self.encoder_layer4(x) #128, 14, 14\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn512(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 128, 7, 7\n",
    "        \n",
    "        x = self.encoder_layer5(x) #256, 7, 7\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn512(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 256, 3, 3\n",
    "        \n",
    "        x = self.encoder_layer6(x) #512, 3, 3\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn512(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) # 512, 1, 1\n",
    "        \n",
    "        x = torch.reshape(x, (-1, 512))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def decoder_pose_reconstructor(self, x):\n",
    "        x = torch.reshape(x, (-1, 512, 1, 1))\n",
    "        \n",
    "        x = self.upsample4(x) # 512, 4, 4\n",
    "        x = self.decoder_layer1(x) # 256, 3, 3\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn256(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.upsample3(x) # 256, 9, 9\n",
    "        x = self.decoder_layer2(x) # 128, 7, 7\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn256(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.upsample2(x) # 128, 14, 14\n",
    "        x = self.decoder_layer3(x) # 64, 14, 14\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn256(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.upsample2(x) # 64, 28, 28\n",
    "        x = self.decoder_layer4(x) # 16, 28, 28\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn64(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.upsample2(x) # 16, 56, 56\n",
    "        x = self.decoder_layer5(x) # 4, 56, 56\n",
    "        x = F.dropout(x, p=0.4)\n",
    "        # x = self.bn8(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.upsample2(x) # 4, 112, 112\n",
    "        x = self.decoder_layer6(x) # 1, 112, 112\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(self, pose_img, magface_embedding):\n",
    "        coded = self.encoder(pose_img)\n",
    "        reconstructed_pose = self.decoder_pose_reconstructor(coded)\n",
    "        \n",
    "        # Feature fusion\n",
    "        generated_embedding = self.decoder_emb_generator(coded, magface_embedding)\n",
    "        \n",
    "        # fc_out = self.fc_head(generated_embedding)\n",
    "        \n",
    "#         id_predictions = self.id_head(fc_out)\n",
    "#         id_predictions = F.softmax(id_predictions, dim=1)\n",
    "        id_predictions = None\n",
    "        \n",
    "#         emo_predictions = self.emotion_head(fc_out)\n",
    "#         emo_predictions = F.softmax(emo_predictions, dim=1)\n",
    "        emo_predictions = None\n",
    "        \n",
    "        return reconstructed_pose, generated_embedding, (id_predictions, emo_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ed81d5-9f60-4ee0-a33e-2deae927ca89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvAutoencoder(\n",
       "  (decoder_emb_generator): EmbeddingGeneratorDecoder(\n",
       "    (vit): VisionTransformer(\n",
       "      (input_layer): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (transformer): Sequential(\n",
       "        (0): AttentionBlock(\n",
       "          (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionBlock(\n",
       "          (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): AttentionBlock(\n",
       "          (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mlp_head): Sequential(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (normalizer): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (encoder_layer1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder_layer2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder_layer3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder_layer4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder_layer5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder_layer6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upsample2): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (upsample3): Upsample(scale_factor=3.0, mode=nearest)\n",
       "  (upsample4): Upsample(scale_factor=4.0, mode=nearest)\n",
       "  (decoder_layer1): Conv2d(512, 256, kernel_size=(2, 2), stride=(3, 3), padding=(2, 2))\n",
       "  (decoder_layer2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (decoder_layer3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (decoder_layer4): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (decoder_layer5): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (decoder_layer6): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ConvAutoencoder()\n",
    "net = net.to(\"cuda\")\n",
    "state_dict = torch.load(\"./augmentor.pt\")\n",
    "net.load_state_dict(state_dict)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c76da726-1d1c-4ba0-b436-044d7d0d066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [12:54<00:00, 12.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "objective1_traindata_embs = []\n",
    "objective1_traindata_labels_pose = []\n",
    "objective1_traindata_labels_id = []\n",
    "objective1_traindata_labels_emo = []\n",
    "objective1_valdata_embs = []\n",
    "objective1_valdata_labels_pose = []\n",
    "objective1_valdata_labels_id = []\n",
    "objective1_valdata_labels_emo = []\n",
    "\n",
    "objective2_traindata_embs = []\n",
    "objective2_traindata_labels_pose = []\n",
    "objective2_traindata_labels_id = []\n",
    "objective2_traindata_labels_emo = []\n",
    "objective2_valdata_embs = []\n",
    "objective2_valdata_labels_pose = []\n",
    "objective2_valdata_labels_id = []\n",
    "objective2_valdata_labels_emo = []\n",
    "\n",
    "objective3_traindata_embs = []\n",
    "objective3_traindata_labels_pose = []\n",
    "objective3_traindata_labels_id = []\n",
    "objective3_traindata_labels_emo = []\n",
    "objective3_valdata_embs = []\n",
    "objective3_valdata_labels_pose = []\n",
    "objective3_valdata_labels_id = []\n",
    "objective3_valdata_labels_emo = []\n",
    "\n",
    "for data in tqdm.tqdm(val_dataset):\n",
    "    pose_img = data[0]\n",
    "    pose_img = torch.Tensor(pose_img)\n",
    "    pose_img = pose_img.unsqueeze(0)\n",
    "    pose_img = pose_img.permute(0, 3, 1, 2)\n",
    "    pose_img = pose_img.to(\"cuda\").to(torch.float32)\n",
    "    \n",
    "    input_emb = data[1]\n",
    "    objective1_traindata_embs.append(input_emb)\n",
    "    objective2_traindata_embs.append(input_emb)\n",
    "    objective3_traindata_embs.append(input_emb)\n",
    "    input_emb = torch.Tensor(input_emb)\n",
    "    input_emb = input_emb.unsqueeze(0)\n",
    "    input_emb = input_emb.to(\"cuda\").to(torch.float32)\n",
    "    \n",
    "    output_emb = data[2]\n",
    "    output_emb = torch.Tensor(output_emb)\n",
    "    output_emb = output_emb.unsqueeze(0)\n",
    "    output_emb = output_emb.to(\"cuda\").to(torch.float32)\n",
    "    \n",
    "    expected_pose_label = data[3]\n",
    "    input_pose_label = data[4]\n",
    "    objective1_traindata_labels_pose.append(input_pose_label)\n",
    "    objective2_traindata_labels_pose.append(input_pose_label)\n",
    "    objective3_traindata_labels_pose.append(input_pose_label)\n",
    "    \n",
    "    expected_id_label = data[5]\n",
    "    objective1_traindata_labels_id.append(expected_id_label)\n",
    "    objective2_traindata_labels_id.append(expected_id_label)\n",
    "    objective3_traindata_labels_id.append(expected_id_label)\n",
    "    \n",
    "    expected_emo_label = data[6]\n",
    "    objective1_traindata_labels_emo.append(expected_emo_label)\n",
    "    objective2_traindata_labels_emo.append(expected_emo_label)\n",
    "    objective3_traindata_labels_emo.append(expected_emo_label)\n",
    "    \n",
    "    # ensure to not mixing the train and validation sets\n",
    "    output_emb = output_emb.detach().cpu().numpy()\n",
    "    \n",
    "    features = net.encoder(pose_img)\n",
    "    generated_embedding = net.decoder_emb_generator(features, input_emb)\n",
    "    \n",
    "    generated_embedding = generated_embedding.detach().cpu().numpy()\n",
    "    \n",
    "    # objective 1: measuring how good separable are the representations\n",
    "    objective1_valdata_embs.append(generated_embedding[0]) \n",
    "    objective1_valdata_labels_pose.append(expected_pose_label) \n",
    "    objective1_valdata_labels_id.append(expected_id_label) \n",
    "    objective1_valdata_labels_emo.append(expected_emo_label)\n",
    "    \n",
    "    # objective1_traindata_embs.append(output_emb[0])\n",
    "    # objective1_traindata_labels_pose.append(expected_pose_label)\n",
    "    # objective1_traindata_labels_id.append(expected_id_label)\n",
    "    # objective1_traindata_labels_emo.append(expected_emo_label)\n",
    "    \n",
    "    # objective 2 and 3: measuring how good the generated representations improve our accuracy\n",
    "    objective2_valdata_embs.append(output_emb[0])\n",
    "    objective2_valdata_labels_pose.append(expected_pose_label)\n",
    "    objective2_valdata_labels_id.append(expected_id_label)\n",
    "    objective2_valdata_labels_emo.append(expected_emo_label)\n",
    "    \n",
    "    objective3_traindata_embs.append(generated_embedding[0])\n",
    "    objective3_traindata_labels_pose.append(expected_pose_label)\n",
    "    objective3_traindata_labels_id.append(expected_id_label)\n",
    "    objective3_traindata_labels_emo.append(expected_emo_label)\n",
    "    \n",
    "    objective3_valdata_embs.append(output_emb[0])\n",
    "    objective3_valdata_labels_pose.append(expected_pose_label)\n",
    "    objective3_valdata_labels_id.append(expected_id_label)\n",
    "    objective3_valdata_labels_emo.append(expected_emo_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33eb4968-32cf-42cb-8bff-307c489f4c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective1_traindata_labels_pose (array([0, 1, 2, 3, 4]), array([  54,   90, 6576, 6703, 6579]))\n",
      "objective1_traindata_labels_id (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]), array([708, 754, 734, 682, 686, 692, 672, 646, 752, 742, 810, 704, 700,\n",
      "       740, 698, 678, 758, 700, 688, 748, 674, 736, 660, 694, 734, 732,\n",
      "       700, 780]))\n",
      "objective1_traindata_labels_emo (array([0, 1, 2, 3, 4, 5, 6]), array([2852, 2748, 2888, 2946, 2974, 2694, 2900]))\n",
      "objective1_valdata_labels_pose (array([0, 1, 2, 3, 4]), array([   3,    7, 3244, 3434, 3313]))\n",
      "objective1_valdata_labels_id (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]), array([354, 377, 367, 341, 343, 346, 336, 323, 376, 371, 405, 352, 350,\n",
      "       370, 349, 339, 379, 350, 344, 374, 337, 368, 330, 347, 367, 366,\n",
      "       350, 390]))\n",
      "objective1_valdata_labels_emo (array([0, 1, 2, 3, 4, 5, 6]), array([1426, 1374, 1444, 1473, 1487, 1347, 1450]))\n"
     ]
    }
   ],
   "source": [
    "print(\"objective1_traindata_labels_pose\", np.unique(objective1_traindata_labels_pose, return_counts=True))\n",
    "print(\"objective1_traindata_labels_id\", np.unique(objective1_traindata_labels_id, return_counts=True))\n",
    "print(\"objective1_traindata_labels_emo\", np.unique(objective1_traindata_labels_emo, return_counts=True))\n",
    "\n",
    "print(\"objective1_valdata_labels_pose\", np.unique(objective1_valdata_labels_pose, return_counts=True))\n",
    "print(\"objective1_valdata_labels_id\", np.unique(objective1_valdata_labels_id, return_counts=True))\n",
    "print(\"objective1_valdata_labels_emo\", np.unique(objective1_valdata_labels_emo, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a720ba4-90ec-4da4-bf94-70f9f1f92b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective1_traindata_embs = np.array(objective1_traindata_embs)\n",
    "objective1_valdata_embs = np.array(objective1_valdata_embs)\n",
    "\n",
    "objective2_traindata_embs = np.array(objective2_traindata_embs)\n",
    "objective2_valdata_embs = np.array(objective2_valdata_embs)\n",
    "\n",
    "objective3_traindata_embs = np.array(objective3_traindata_embs)\n",
    "objective3_valdata_embs = np.array(objective3_valdata_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de0fe510-8eec-4521-817b-c32d39a15b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(objective1_traindata_embs)\n",
    "objective1_traindata_embs = scaler.transform(objective1_traindata_embs)\n",
    "scaler.fit(objective2_traindata_embs)\n",
    "objective2_traindata_embs = scaler.transform(objective2_traindata_embs)\n",
    "scaler.fit(objective3_traindata_embs)\n",
    "objective3_traindata_embs = scaler.transform(objective3_traindata_embs)\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "scaler2.fit(objective1_valdata_embs)\n",
    "objective1_valdata_embs = scaler2.transform(objective1_valdata_embs)\n",
    "scaler2.fit(objective2_valdata_embs)\n",
    "objective2_valdata_embs = scaler2.transform(objective2_valdata_embs)\n",
    "scaler2.fit(objective3_valdata_embs)\n",
    "objective3_valdata_embs = scaler2.transform(objective3_valdata_embs)\n",
    "\n",
    "le_pose = LabelEncoder()\n",
    "le_id = LabelEncoder()\n",
    "le_emo = LabelEncoder()\n",
    "objective1_traindata_labels_encoded_pose = le_pose.fit_transform(objective1_traindata_labels_pose)\n",
    "objective1_traindata_labels_encoded_id = le_id.fit_transform(objective1_traindata_labels_id)\n",
    "objective1_traindata_labels_encoded_emo = le_emo.fit_transform(objective1_traindata_labels_emo)\n",
    "objective1_valdata_labels_encoded_pose = le_pose.transform(objective1_valdata_labels_pose)\n",
    "objective1_valdata_labels_encoded_id = le_id.transform(objective1_valdata_labels_id)\n",
    "objective1_valdata_labels_encoded_emo = le_emo.transform(objective1_valdata_labels_emo)\n",
    "\n",
    "le_pose = LabelEncoder()\n",
    "le_id = LabelEncoder()\n",
    "le_emo = LabelEncoder()\n",
    "objective2_traindata_labels_encoded_pose = le_pose.fit_transform(objective2_traindata_labels_pose)\n",
    "objective2_traindata_labels_encoded_id = le_id.fit_transform(objective2_traindata_labels_id)\n",
    "objective2_traindata_labels_encoded_emo = le_emo.fit_transform(objective2_traindata_labels_emo)\n",
    "objective2_valdata_labels_encoded_pose = le_pose.transform(objective2_valdata_labels_pose)\n",
    "objective2_valdata_labels_encoded_id = le_id.transform(objective2_valdata_labels_id)\n",
    "objective2_valdata_labels_encoded_emo = le_emo.transform(objective2_valdata_labels_emo)\n",
    "\n",
    "le_pose = LabelEncoder()\n",
    "le_id = LabelEncoder()\n",
    "le_emo = LabelEncoder()\n",
    "objective3_traindata_labels_encoded_pose = le_pose.fit_transform(objective3_traindata_labels_pose)\n",
    "objective3_traindata_labels_encoded_id = le_id.fit_transform(objective3_traindata_labels_id)\n",
    "objective3_traindata_labels_encoded_emo = le_emo.fit_transform(objective3_traindata_labels_emo)\n",
    "objective3_valdata_labels_encoded_pose = le_pose.transform(objective3_valdata_labels_pose)\n",
    "objective3_valdata_labels_encoded_id = le_id.transform(objective3_valdata_labels_id)\n",
    "objective3_valdata_labels_encoded_emo = le_emo.transform(objective3_valdata_labels_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0efbae46-2827-4261-9d8c-1d03bfdfb38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective 1\n",
      "1.0 1.0 1.0\n",
      "0.6073392660733926 0.7758224177582241 0.40135986401359863\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "accuracy_pose = []\n",
    "accuracy_id = []\n",
    "accuracy_emotion = []\n",
    "val_accuracy_pose = []\n",
    "val_accuracy_id = []\n",
    "val_accuracy_emotion = []\n",
    "\n",
    "clf_pose = SVC()\n",
    "clf_pose.fit(objective1_traindata_embs, objective1_traindata_labels_encoded_pose)\n",
    "accuracy_pose.append(np.mean(clf_pose.predict(objective1_traindata_embs) == objective1_traindata_labels_encoded_pose))\n",
    "val_accuracy_pose.append(np.mean(clf_pose.predict(objective1_valdata_embs) == objective1_valdata_labels_encoded_pose))\n",
    "\n",
    "clf_id = SVC()\n",
    "clf_id.fit(objective1_traindata_embs, objective1_traindata_labels_encoded_id)\n",
    "accuracy_id.append(np.mean(clf_id.predict(objective1_traindata_embs) == objective1_traindata_labels_encoded_id))\n",
    "val_accuracy_id.append(np.mean(clf_id.predict(objective1_valdata_embs) == objective1_valdata_labels_encoded_id))\n",
    "\n",
    "clf_emo = SVC()\n",
    "clf_emo.fit(objective1_traindata_embs, objective1_traindata_labels_encoded_emo)\n",
    "accuracy_emotion.append(np.mean(clf_emo.predict(objective1_traindata_embs) == objective1_traindata_labels_encoded_emo))\n",
    "val_accuracy_emotion.append(np.mean(clf_emo.predict(objective1_valdata_embs) == objective1_valdata_labels_encoded_emo))\n",
    "\n",
    "print(\"objective 1\")\n",
    "print(np.mean(accuracy_pose), np.mean(accuracy_id), np.mean(accuracy_emotion))\n",
    "print(np.mean(val_accuracy_pose), np.mean(val_accuracy_id), np.mean(val_accuracy_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0011088-bde6-467f-8c91-2e4eae4c1dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective 2\n",
      "1.0 1.0 1.0\n",
      "1.0 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "accuracy_pose = []\n",
    "accuracy_id = []\n",
    "accuracy_emotion = []\n",
    "val_accuracy_pose = []\n",
    "val_accuracy_id = []\n",
    "val_accuracy_emotion = []\n",
    "\n",
    "clf_pose = SVC()\n",
    "clf_pose.fit(objective2_traindata_embs, objective2_traindata_labels_encoded_pose)\n",
    "accuracy_pose.append(np.mean(clf_pose.predict(objective2_traindata_embs) == objective2_traindata_labels_encoded_pose))\n",
    "val_accuracy_pose.append(np.mean(clf_pose.predict(objective2_valdata_embs) == objective2_valdata_labels_encoded_pose))\n",
    "\n",
    "clf_id = SVC()\n",
    "clf_id.fit(objective2_traindata_embs, objective2_traindata_labels_encoded_id)\n",
    "accuracy_id.append(np.mean(clf_id.predict(objective2_traindata_embs) == objective2_traindata_labels_encoded_id))\n",
    "val_accuracy_id.append(np.mean(clf_id.predict(objective2_valdata_embs) == objective2_valdata_labels_encoded_id))\n",
    "\n",
    "clf_emo = SVC()\n",
    "clf_emo.fit(objective2_traindata_embs, objective2_traindata_labels_encoded_emo)\n",
    "accuracy_emotion.append(np.mean(clf_emo.predict(objective2_traindata_embs) == objective2_traindata_labels_encoded_emo))\n",
    "val_accuracy_emotion.append(np.mean(clf_emo.predict(objective2_valdata_embs) == objective2_valdata_labels_encoded_emo))\n",
    "\n",
    "print(\"objective 2\")\n",
    "print(np.mean(accuracy_pose), np.mean(accuracy_id), np.mean(accuracy_emotion))\n",
    "print(np.mean(val_accuracy_pose), np.mean(val_accuracy_id), np.mean(val_accuracy_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b2e9103-8dc6-4fd6-93d1-4adcce1292cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective 3\n",
      "0.9295070492950704 0.978952104789521 0.9911508849115088\n",
      "1.0 0.9981001899810019 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "accuracy_pose = []\n",
    "accuracy_id = []\n",
    "accuracy_emotion = []\n",
    "val_accuracy_pose = []\n",
    "val_accuracy_id = []\n",
    "val_accuracy_emotion = []\n",
    "\n",
    "clf_pose = SVC()\n",
    "clf_pose.fit(objective3_traindata_embs, objective3_traindata_labels_encoded_pose)\n",
    "accuracy_pose.append(np.mean(clf_pose.predict(objective3_traindata_embs) == objective3_traindata_labels_encoded_pose))\n",
    "val_accuracy_pose.append(np.mean(clf_pose.predict(objective3_valdata_embs) == objective3_valdata_labels_encoded_pose))\n",
    "\n",
    "clf_id = SVC()\n",
    "clf_id.fit(objective3_traindata_embs, objective3_traindata_labels_encoded_id)\n",
    "accuracy_id.append(np.mean(clf_id.predict(objective3_traindata_embs) == objective3_traindata_labels_encoded_id))\n",
    "val_accuracy_id.append(np.mean(clf_id.predict(objective3_valdata_embs) == objective3_valdata_labels_encoded_id))\n",
    "\n",
    "clf_emo = SVC()\n",
    "clf_emo.fit(objective3_traindata_embs, objective3_traindata_labels_encoded_emo)\n",
    "accuracy_emotion.append(np.mean(clf_emo.predict(objective3_traindata_embs) == objective3_traindata_labels_encoded_emo))\n",
    "val_accuracy_emotion.append(np.mean(clf_emo.predict(objective3_valdata_embs) == objective3_valdata_labels_encoded_emo))\n",
    "\n",
    "print(\"objective 3\")\n",
    "print(np.mean(accuracy_pose), np.mean(accuracy_id), np.mean(accuracy_emotion))\n",
    "print(np.mean(val_accuracy_pose), np.mean(val_accuracy_id), np.mean(val_accuracy_emotion))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
