{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9c0be6-2dbc-4ad6-9a36-76ba97814e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.image import extract_face_landmarks\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70deb9b8-c882-4f64-8f3a-1fa956a83c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class CustomDataSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, type_=\"train\"):\n",
    "        self.type_ = type_\n",
    "        \n",
    "        all_files = list(set([re.findall(r\"(.+)_(\\d+)_(.+)_(\\d+)_(\\d+)_.+\", filename)[0] for filename in os.listdir(f\"/media/soroushh/Storage2/matrices/evaluation\")]))\n",
    "        if self.type_ == \"train\":\n",
    "            all_files = all_files[:int((2/3) * len(all_files))]\n",
    "        else:\n",
    "            all_files = all_files[int((2/3) * len(all_files)):]\n",
    "        \n",
    "        self.final_database = []\n",
    "        for person1_name, triple_to_reconstruct_index, person2_name, triple_to_change_input_index, triple_to_change_output_index in all_files:\n",
    "            triple_to_reconstruct_index = int(triple_to_reconstruct_index)\n",
    "            triple_to_change_input_index = int(triple_to_change_input_index)\n",
    "            triple_to_change_output_index = int(triple_to_change_output_index)\n",
    "            self.final_database.append((person1_name, triple_to_reconstruct_index, person2_name, triple_to_change_input_index, triple_to_change_output_index))\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/database_evaluation.pickle', 'rb') as handle:\n",
    "            self.database = pickle.load(handle)\n",
    "        \n",
    "        ids = list(self.database.keys())\n",
    "        self.ids_dict = {id:idx for idx, id in enumerate(list(self.database.keys()))}\n",
    "        self.emotions_dict = {emo:idx for idx, emo in enumerate(np.unique([triplet[0] for id in ids for triplet in self.database[id]]).tolist())}\n",
    "        self.poses_dict = {pose:idx for idx, pose in enumerate(np.unique([triplet[1] for id in ids for triplet in self.database[id]]).tolist())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.final_database)\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        tup = self.final_database[idx]\n",
    "        \n",
    "        with open(f'/media/soroushh/Storage2/matrices/evaluation/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_pose_img_to_reconstruct.npy', 'rb') as f:\n",
    "            pose_img_to_reconstruct = np.load(f)\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/matrices/evaluation/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_input.npy', 'rb') as f:\n",
    "            embedding_input = np.load(f)\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/matrices/evaluation/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_output.npy', 'rb') as f:\n",
    "            embedding_output = np.load(f)\n",
    "            \n",
    "        expected_pose_label = self.database[tup[2]][tup[4]][1]\n",
    "        input_pose_label = self.database[tup[2]][tup[3]][1]\n",
    "        input_id = tup[2]\n",
    "        input_emo = self.database[tup[2]][tup[3]][0]\n",
    "        \n",
    "        while True:\n",
    "            available_negative_choices = list(filter(lambda item: item[2] != input_id, self.final_database))\n",
    "            if len(available_negative_choices) == 0:\n",
    "                continue\n",
    "                \n",
    "            neg_tup = random.choice(available_negative_choices)\n",
    "            break\n",
    "            \n",
    "        with open(f'/media/soroushh/Storage2/matrices/evaluation/{neg_tup[0]}_{neg_tup[1]}_{neg_tup[2]}_{neg_tup[3]}_{neg_tup[4]}_embedding_input.npy', 'rb') as f:\n",
    "            negative_embedding_input = np.load(f)\n",
    "            \n",
    "        expected_pose_label = self.poses_dict[expected_pose_label]\n",
    "        input_pose_label = self.poses_dict[input_pose_label]\n",
    "        input_id = self.ids_dict[input_id]\n",
    "        input_emo = self.emotions_dict[input_emo]\n",
    "            \n",
    "        return pose_img_to_reconstruct, embedding_input, embedding_output, expected_pose_label, input_pose_label, input_id, input_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62111018-53a1-4653-9c41-c73661099638",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob.glob(os.path.join(\"./KDEF/*/*.JPG\"))\n",
    "image_paths = np.array(image_paths)\n",
    "np.random.shuffle(image_paths)\n",
    "image_paths = image_paths.tolist()\n",
    "\n",
    "index = int(0.9 * len(image_paths))\n",
    "# trainset = image_paths[:index]\n",
    "valset = image_paths[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad90db3-ada3-48e5-a071-3b26629f949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CustomDataSet(trainset, type_=\"train\", precomputerd=True)\n",
    "val_dataset = CustomDataSet(type_=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0e9b7c-a10c-46cf-8c98-30515adf84bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_dataset), \n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ed81d5-9f60-4ee0-a33e-2deae927ca89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvAutoencoder(\n",
       "  (decoder_emb_generator): EmbeddingGeneratorDecoder(\n",
       "    (vit): VisionTransformer(\n",
       "      (input_layer): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (transformer): Sequential(\n",
       "        (0): AttentionBlock(\n",
       "          (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): AttentionBlock(\n",
       "          (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionBlock(\n",
       "          (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): AttentionBlock(\n",
       "          (layer_norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (layer_norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mlp_head): Sequential(\n",
       "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (normalizer): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (encoder_layer1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder_layer2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder_layer3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder_layer4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder_layer5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder_layer6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upsample2): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (upsample3): Upsample(scale_factor=3.0, mode=nearest)\n",
       "  (upsample4): Upsample(scale_factor=4.0, mode=nearest)\n",
       "  (decoder_layer1): Conv2d(512, 256, kernel_size=(2, 2), stride=(3, 3), padding=(2, 2))\n",
       "  (decoder_layer2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (decoder_layer3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (decoder_layer4): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (decoder_layer5): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (decoder_layer6): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ConvAutoencoder()\n",
    "net = net.to(\"cuda\")\n",
    "state_dict = torch.load(\"./augmentor.pt\")\n",
    "net.load_state_dict(state_dict)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea8408-3b81-4bec-b388-d7af3e65edcb",
   "metadata": {},
   "source": [
    "# Visualize AE reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00689798-48b1-41e5-b631-0fca5a4e474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in train_dataloader:\n",
    "        pose_img = data[0]\n",
    "        pose_img = pose_img.permute(0, 3, 1, 2)\n",
    "        pose_img = pose_img.to(\"cuda\").to(torch.float32)\n",
    "        input_emb = data[1]\n",
    "        input_emb = input_emb.to(\"cuda\").to(torch.float32)\n",
    "        output_emb = data[2]\n",
    "        output_emb = output_emb.to(\"cuda\").to(torch.float32)\n",
    "        \n",
    "        features = net.encoder(pose_img)\n",
    "        \n",
    "        reconstructed_data = net.decoder_pose_reconstructor(features)\n",
    "        \n",
    "        reconstructed_data = reconstructed_data.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        orig_data = pose_img.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        \n",
    "        for i in range(reconstructed_data.shape[0]):\n",
    "            plt.imshow(orig_data[i, :, :, 0])\n",
    "            plt.show()\n",
    "            plt.imshow(reconstructed_data[i, :, :, 0])\n",
    "            plt.show()\n",
    "            \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45786f9-c631-47c5-88df-b5430a0435f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in val_dataloader:\n",
    "        pose_img = data[0]\n",
    "        pose_img = pose_img.permute(0, 3, 1, 2)\n",
    "        pose_img = pose_img.to(\"cuda\").to(torch.float32)\n",
    "        input_emb = data[1]\n",
    "        input_emb = input_emb.to(\"cuda\").to(torch.float32)\n",
    "        output_emb = data[2]\n",
    "        output_emb = output_emb.to(\"cuda\").to(torch.float32)\n",
    "        \n",
    "        features, _ = net.encoder(pose_img)\n",
    "        \n",
    "        reconstructed_data = net.decoder_pose_reconstructor(features)\n",
    "        \n",
    "        reconstructed_data = reconstructed_data.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        orig_data = pose_img.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        \n",
    "        for i in range(reconstructed_data.shape[0]):\n",
    "            plt.imshow(orig_data[i, :, :, 0])\n",
    "            plt.show()\n",
    "            plt.imshow(reconstructed_data[i, :, :, 0])\n",
    "            plt.show()\n",
    "            \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378db180-5fc1-4fa0-980d-9a4857b899a5",
   "metadata": {},
   "source": [
    "# Evaluate quality of generated representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c76da726-1d1c-4ba0-b436-044d7d0d066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10001/10001 [16:51<00:00,  9.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "objective1_traindata_embs = []\n",
    "objective1_traindata_labels_pose = []\n",
    "objective1_traindata_labels_id = []\n",
    "objective1_traindata_labels_emo = []\n",
    "objective1_valdata_embs = []\n",
    "objective1_valdata_labels_pose = []\n",
    "objective1_valdata_labels_id = []\n",
    "objective1_valdata_labels_emo = []\n",
    "\n",
    "objective2_traindata_embs = []\n",
    "objective2_traindata_labels_pose = []\n",
    "objective2_traindata_labels_id = []\n",
    "objective2_traindata_labels_emo = []\n",
    "objective2_valdata_embs = []\n",
    "objective2_valdata_labels_pose = []\n",
    "objective2_valdata_labels_id = []\n",
    "objective2_valdata_labels_emo = []\n",
    "\n",
    "objective3_traindata_embs = []\n",
    "objective3_traindata_labels_pose = []\n",
    "objective3_traindata_labels_id = []\n",
    "objective3_traindata_labels_emo = []\n",
    "objective3_valdata_embs = []\n",
    "objective3_valdata_labels_pose = []\n",
    "objective3_valdata_labels_id = []\n",
    "objective3_valdata_labels_emo = []\n",
    "\n",
    "for data in tqdm.tqdm(val_dataset):\n",
    "    pose_img = data[0]\n",
    "    pose_img = torch.Tensor(pose_img)\n",
    "    pose_img = pose_img.unsqueeze(0)\n",
    "    pose_img = pose_img.permute(0, 3, 1, 2)\n",
    "    pose_img = pose_img.to(\"cuda\").to(torch.float32)\n",
    "    \n",
    "    input_emb = data[1]\n",
    "    objective1_traindata_embs.append(input_emb)\n",
    "    objective2_traindata_embs.append(input_emb)\n",
    "    objective3_traindata_embs.append(input_emb)\n",
    "    input_emb = torch.Tensor(input_emb)\n",
    "    input_emb = input_emb.unsqueeze(0)\n",
    "    input_emb = input_emb.to(\"cuda\").to(torch.float32)\n",
    "    \n",
    "    output_emb = data[2]\n",
    "    output_emb = torch.Tensor(output_emb)\n",
    "    output_emb = output_emb.unsqueeze(0)\n",
    "    output_emb = output_emb.to(\"cuda\").to(torch.float32)\n",
    "    \n",
    "    expected_pose_label = data[3]\n",
    "    input_pose_label = data[4]\n",
    "    objective1_traindata_labels_pose.append(input_pose_label)\n",
    "    objective2_traindata_labels_pose.append(input_pose_label)\n",
    "    objective3_traindata_labels_pose.append(input_pose_label)\n",
    "    \n",
    "    expected_id_label = data[5]\n",
    "    objective1_traindata_labels_id.append(expected_id_label)\n",
    "    objective2_traindata_labels_id.append(expected_id_label)\n",
    "    objective3_traindata_labels_id.append(expected_id_label)\n",
    "    \n",
    "    expected_emo_label = data[6]\n",
    "    objective1_traindata_labels_emo.append(expected_emo_label)\n",
    "    objective2_traindata_labels_emo.append(expected_emo_label)\n",
    "    objective3_traindata_labels_emo.append(expected_emo_label)\n",
    "    \n",
    "    # ensure to not mixing the train and validation sets\n",
    "    output_emb = output_emb.detach().cpu().numpy()\n",
    "    \n",
    "    features = net.encoder(pose_img)\n",
    "    generated_embedding = net.decoder_emb_generator(features, input_emb)\n",
    "    \n",
    "    generated_embedding = generated_embedding.detach().cpu().numpy()\n",
    "    \n",
    "    # objective 1: measuring how linear separable are the representations\n",
    "    objective1_valdata_embs.append(generated_embedding[0]) \n",
    "    objective1_valdata_labels_pose.append(expected_pose_label) \n",
    "    objective1_valdata_labels_id.append(expected_id_label) \n",
    "    objective1_valdata_labels_emo.append(expected_emo_label)\n",
    "    \n",
    "    # objective 2 and 3: measuring how good the generated representations improve our accuracy\n",
    "    objective2_valdata_embs.append(output_emb[0])\n",
    "    objective2_valdata_labels_pose.append(expected_pose_label)\n",
    "    objective2_valdata_labels_id.append(expected_id_label)\n",
    "    objective2_valdata_labels_emo.append(expected_emo_label)\n",
    "    \n",
    "    objective3_traindata_embs.append(generated_embedding[0])\n",
    "    objective3_traindata_labels_pose.append(expected_pose_label)\n",
    "    objective3_traindata_labels_id.append(expected_id_label)\n",
    "    objective3_traindata_labels_emo.append(expected_emo_label)\n",
    "    \n",
    "    objective3_valdata_embs.append(output_emb[0])\n",
    "    objective3_valdata_labels_pose.append(expected_pose_label)\n",
    "    objective3_valdata_labels_id.append(expected_id_label)\n",
    "    objective3_valdata_labels_emo.append(expected_emo_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33eb4968-32cf-42cb-8bff-307c489f4c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective1_traindata_labels_pose (array([0, 1, 2, 3, 4]), array([  51,   83, 3306, 3297, 3264]))\n",
      "objective1_traindata_labels_id (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]), array([341, 407, 367, 350, 359, 354, 360, 339, 334, 360, 399, 356, 325,\n",
      "       366, 342, 383, 370, 337, 328, 359, 346, 334, 346, 386, 383, 353,\n",
      "       320, 397]))\n",
      "objective1_traindata_labels_emo (array([0, 1, 2, 3, 4, 5, 6]), array([1429, 1345, 1487, 1439, 1472, 1388, 1441]))\n",
      "objective1_valdata_labels_pose (array([0, 1, 2, 3, 4]), array([   5,    7, 3219, 3401, 3369]))\n",
      "objective1_valdata_labels_id (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]), array([341, 407, 367, 350, 359, 354, 360, 339, 334, 360, 399, 356, 325,\n",
      "       366, 342, 383, 370, 337, 328, 359, 346, 334, 346, 386, 383, 353,\n",
      "       320, 397]))\n",
      "objective1_valdata_labels_emo (array([0, 1, 2, 3, 4, 5, 6]), array([1429, 1345, 1487, 1439, 1472, 1388, 1441]))\n"
     ]
    }
   ],
   "source": [
    "print(\"objective1_traindata_labels_pose\", np.unique(objective1_traindata_labels_pose, return_counts=True))\n",
    "print(\"objective1_traindata_labels_id\", np.unique(objective1_traindata_labels_id, return_counts=True))\n",
    "print(\"objective1_traindata_labels_emo\", np.unique(objective1_traindata_labels_emo, return_counts=True))\n",
    "\n",
    "print(\"objective1_valdata_labels_pose\", np.unique(objective1_valdata_labels_pose, return_counts=True))\n",
    "print(\"objective1_valdata_labels_id\", np.unique(objective1_valdata_labels_id, return_counts=True))\n",
    "print(\"objective1_valdata_labels_emo\", np.unique(objective1_valdata_labels_emo, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a720ba4-90ec-4da4-bf94-70f9f1f92b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective1_traindata_embs = np.array(objective1_traindata_embs)\n",
    "objective1_valdata_embs = np.array(objective1_valdata_embs)\n",
    "\n",
    "objective2_traindata_embs = np.array(objective2_traindata_embs)\n",
    "objective2_valdata_embs = np.array(objective2_valdata_embs)\n",
    "\n",
    "objective3_traindata_embs = np.array(objective3_traindata_embs)\n",
    "objective3_valdata_embs = np.array(objective3_valdata_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de0fe510-8eec-4521-817b-c32d39a15b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(objective1_traindata_embs)\n",
    "objective1_traindata_embs = scaler.transform(objective1_traindata_embs)\n",
    "scaler.fit(objective2_traindata_embs)\n",
    "objective2_traindata_embs = scaler.transform(objective2_traindata_embs)\n",
    "scaler.fit(objective3_traindata_embs)\n",
    "objective3_traindata_embs = scaler.transform(objective3_traindata_embs)\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "scaler2.fit(objective1_valdata_embs)\n",
    "objective1_valdata_embs = scaler2.transform(objective1_valdata_embs)\n",
    "scaler2.fit(objective2_valdata_embs)\n",
    "objective2_valdata_embs = scaler2.transform(objective2_valdata_embs)\n",
    "scaler2.fit(objective3_valdata_embs)\n",
    "objective3_valdata_embs = scaler2.transform(objective3_valdata_embs)\n",
    "\n",
    "le_pose = LabelEncoder()\n",
    "le_id = LabelEncoder()\n",
    "le_emo = LabelEncoder()\n",
    "objective1_traindata_labels_encoded_pose = le_pose.fit_transform(objective1_traindata_labels_pose)\n",
    "objective1_traindata_labels_encoded_id = le_id.fit_transform(objective1_traindata_labels_id)\n",
    "objective1_traindata_labels_encoded_emo = le_emo.fit_transform(objective1_traindata_labels_emo)\n",
    "objective1_valdata_labels_encoded_pose = le_pose.transform(objective1_valdata_labels_pose)\n",
    "objective1_valdata_labels_encoded_id = le_id.transform(objective1_valdata_labels_id)\n",
    "objective1_valdata_labels_encoded_emo = le_emo.transform(objective1_valdata_labels_emo)\n",
    "\n",
    "le_pose = LabelEncoder()\n",
    "le_id = LabelEncoder()\n",
    "le_emo = LabelEncoder()\n",
    "objective2_traindata_labels_encoded_pose = le_pose.fit_transform(objective2_traindata_labels_pose)\n",
    "objective2_traindata_labels_encoded_id = le_id.fit_transform(objective2_traindata_labels_id)\n",
    "objective2_traindata_labels_encoded_emo = le_emo.fit_transform(objective2_traindata_labels_emo)\n",
    "objective2_valdata_labels_encoded_pose = le_pose.transform(objective2_valdata_labels_pose)\n",
    "objective2_valdata_labels_encoded_id = le_id.transform(objective2_valdata_labels_id)\n",
    "objective2_valdata_labels_encoded_emo = le_emo.transform(objective2_valdata_labels_emo)\n",
    "\n",
    "le_pose = LabelEncoder()\n",
    "le_id = LabelEncoder()\n",
    "le_emo = LabelEncoder()\n",
    "objective3_traindata_labels_encoded_pose = le_pose.fit_transform(objective3_traindata_labels_pose)\n",
    "objective3_traindata_labels_encoded_id = le_id.fit_transform(objective3_traindata_labels_id)\n",
    "objective3_traindata_labels_encoded_emo = le_emo.fit_transform(objective3_traindata_labels_emo)\n",
    "objective3_valdata_labels_encoded_pose = le_pose.transform(objective3_valdata_labels_pose)\n",
    "objective3_valdata_labels_encoded_id = le_id.transform(objective3_valdata_labels_id)\n",
    "objective3_valdata_labels_encoded_emo = le_emo.transform(objective3_valdata_labels_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0efbae46-2827-4261-9d8c-1d03bfdfb38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective 1\n",
      "1.0 0.9987001299870013 1.0\n",
      "0.9976002399760024 0.7401259874012599 0.5612438756124387\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "accuracy_pose = []\n",
    "accuracy_id = []\n",
    "accuracy_emotion = []\n",
    "val_accuracy_pose = []\n",
    "val_accuracy_id = []\n",
    "val_accuracy_emotion = []\n",
    "\n",
    "clf_pose = SVC()\n",
    "clf_pose.fit(objective1_traindata_embs, objective1_traindata_labels_encoded_pose)\n",
    "accuracy_pose.append(np.mean(clf_pose.predict(objective1_traindata_embs) == objective1_traindata_labels_encoded_pose))\n",
    "val_accuracy_pose.append(np.mean(clf_pose.predict(objective1_valdata_embs) == objective1_valdata_labels_encoded_pose))\n",
    "\n",
    "clf_id = SVC()\n",
    "clf_id.fit(objective1_traindata_embs, objective1_traindata_labels_encoded_id)\n",
    "accuracy_id.append(np.mean(clf_id.predict(objective1_traindata_embs) == objective1_traindata_labels_encoded_id))\n",
    "val_accuracy_id.append(np.mean(clf_id.predict(objective1_valdata_embs) == objective1_valdata_labels_encoded_id))\n",
    "\n",
    "clf_emo = SVC()\n",
    "clf_emo.fit(objective1_traindata_embs, objective1_traindata_labels_encoded_emo)\n",
    "accuracy_emotion.append(np.mean(clf_emo.predict(objective1_traindata_embs) == objective1_traindata_labels_encoded_emo))\n",
    "val_accuracy_emotion.append(np.mean(clf_emo.predict(objective1_valdata_embs) == objective1_valdata_labels_encoded_emo))\n",
    "\n",
    "print(\"objective 1\")\n",
    "print(np.mean(accuracy_pose), np.mean(accuracy_id), np.mean(accuracy_emotion))\n",
    "print(np.mean(val_accuracy_pose), np.mean(val_accuracy_id), np.mean(val_accuracy_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0011088-bde6-467f-8c91-2e4eae4c1dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective 2\n",
      "1.0 0.9987001299870013 1.0\n",
      "1.0 0.9986001399860014 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "accuracy_pose = []\n",
    "accuracy_id = []\n",
    "accuracy_emotion = []\n",
    "val_accuracy_pose = []\n",
    "val_accuracy_id = []\n",
    "val_accuracy_emotion = []\n",
    "\n",
    "clf_pose = SVC()\n",
    "clf_pose.fit(objective2_traindata_embs, objective2_traindata_labels_encoded_pose)\n",
    "accuracy_pose.append(np.mean(clf_pose.predict(objective2_traindata_embs) == objective2_traindata_labels_encoded_pose))\n",
    "val_accuracy_pose.append(np.mean(clf_pose.predict(objective2_valdata_embs) == objective2_valdata_labels_encoded_pose))\n",
    "\n",
    "clf_id = SVC()\n",
    "clf_id.fit(objective2_traindata_embs, objective2_traindata_labels_encoded_id)\n",
    "accuracy_id.append(np.mean(clf_id.predict(objective2_traindata_embs) == objective2_traindata_labels_encoded_id))\n",
    "val_accuracy_id.append(np.mean(clf_id.predict(objective2_valdata_embs) == objective2_valdata_labels_encoded_id))\n",
    "\n",
    "clf_emo = SVC()\n",
    "clf_emo.fit(objective2_traindata_embs, objective2_traindata_labels_encoded_emo)\n",
    "accuracy_emotion.append(np.mean(clf_emo.predict(objective2_traindata_embs) == objective2_traindata_labels_encoded_emo))\n",
    "val_accuracy_emotion.append(np.mean(clf_emo.predict(objective2_valdata_embs) == objective2_valdata_labels_encoded_emo))\n",
    "\n",
    "print(\"objective 2\")\n",
    "print(np.mean(accuracy_pose), np.mean(accuracy_id), np.mean(accuracy_emotion))\n",
    "print(np.mean(val_accuracy_pose), np.mean(val_accuracy_id), np.mean(val_accuracy_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b2e9103-8dc6-4fd6-93d1-4adcce1292cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective 3\n",
      "0.9994000599940006 0.9795020497950205 0.9828017198280172\n",
      "1.0 0.9986001399860014 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "accuracy_pose = []\n",
    "accuracy_id = []\n",
    "accuracy_emotion = []\n",
    "val_accuracy_pose = []\n",
    "val_accuracy_id = []\n",
    "val_accuracy_emotion = []\n",
    "\n",
    "clf_pose = SVC()\n",
    "clf_pose.fit(objective3_traindata_embs, objective3_traindata_labels_encoded_pose)\n",
    "accuracy_pose.append(np.mean(clf_pose.predict(objective3_traindata_embs) == objective3_traindata_labels_encoded_pose))\n",
    "val_accuracy_pose.append(np.mean(clf_pose.predict(objective3_valdata_embs) == objective3_valdata_labels_encoded_pose))\n",
    "\n",
    "clf_id = SVC()\n",
    "clf_id.fit(objective3_traindata_embs, objective3_traindata_labels_encoded_id)\n",
    "accuracy_id.append(np.mean(clf_id.predict(objective3_traindata_embs) == objective3_traindata_labels_encoded_id))\n",
    "val_accuracy_id.append(np.mean(clf_id.predict(objective3_valdata_embs) == objective3_valdata_labels_encoded_id))\n",
    "\n",
    "clf_emo = SVC()\n",
    "clf_emo.fit(objective3_traindata_embs, objective3_traindata_labels_encoded_emo)\n",
    "accuracy_emotion.append(np.mean(clf_emo.predict(objective3_traindata_embs) == objective3_traindata_labels_encoded_emo))\n",
    "val_accuracy_emotion.append(np.mean(clf_emo.predict(objective3_valdata_embs) == objective3_valdata_labels_encoded_emo))\n",
    "\n",
    "print(\"objective 3\")\n",
    "print(np.mean(accuracy_pose), np.mean(accuracy_id), np.mean(accuracy_emotion))\n",
    "print(np.mean(val_accuracy_pose), np.mean(val_accuracy_id), np.mean(val_accuracy_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b1892-3a6d-4042-8e31-445788703dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
