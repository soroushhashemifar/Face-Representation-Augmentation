{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XxH50o1ZKrhq"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install gdown --force-reinstall\n",
    "# !gdown --id 1TEOjzSBN6UZc1WQwb_huEhdFl1XyZel4\n",
    "# !unzip KDEF_CROPPED_ALIGNED.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VjuyB0EsHCqc"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !gdown --id 1Bd87admxOZvbIOAyTkGEntsEz3fyMt7H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install mlxtend\n",
    "# !pip install dlib\n",
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_oDZvCVOTsG",
    "outputId": "837ada7e-7b92-428b-97fb-2069633e073e"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.image import extract_face_landmarks\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzOuZ6dCKbii"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading pth from ./magface_epoch_00025.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3903/3903 [01:09<00:00, 56.01it/s]\n",
      "100%|██████████| 60001/60001 [1:10:53<00:00, 14.11it/s]\n",
      "100%|██████████| 977/977 [00:17<00:00, 55.77it/s]\n",
      "100%|██████████| 49115/49115 [45:55<00:00, 17.82it/s]  \n"
     ]
    }
   ],
   "source": [
    "!rm -rf /media/soroushh/Storage2/matrices\n",
    "!mkdir -p /media/soroushh/Storage2/matrices/training\n",
    "!mkdir -p /media/soroushh/Storage2/matrices/evaluation\n",
    "\n",
    "from magface_model import iresnet100, load_dict_inf\n",
    "\n",
    "magface = iresnet100(pretrained=False, num_classes=512)\n",
    "magface = load_dict_inf(magface, \"./magface_epoch_00025.pth\")\n",
    "magface = magface.to(\"cuda\")\n",
    "magface.eval()\n",
    "\n",
    "def get_landmarks(image_paths, image_size):\n",
    "    database = {}\n",
    "    for image_path in tqdm.tqdm(image_paths):\n",
    "        filename = image_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        if not bool(re.match(r\"\\w{2}\\d{2}\\w{2}\\w+\", filename)):\n",
    "            continue\n",
    "\n",
    "        name = re.findall(r\"(\\w{2}\\d{2})\\w{2}\\w+\", filename)[0]\n",
    "        pose_label = re.findall(r\"\\w{2}\\d{2}\\w{2}(\\w+)\", filename)[0]\n",
    "        emotion_label = re.findall(r\"\\w{2}\\d{2}(\\w{2})\\w+\", filename)[0]\n",
    "\n",
    "        # img = cv2.imread(image_path)\n",
    "        img = cv2.imread(image_path.replace(\"CROPPED_ALIGNED\", \"KDEF\")) # get real images from KDEF for landmarks\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        img = cv2.copyMakeBorder(img, 10, 10, 10, 10, cv2.BORDER_CONSTANT)\n",
    "        pose_img = get_pose_image(img, image_size)\n",
    "\n",
    "        if pose_img is None:\n",
    "            continue\n",
    "\n",
    "        person_db = database.get(name, [])\n",
    "        person_db.append((emotion_label, pose_label, pose_img, image_path))\n",
    "        database[name] = person_db\n",
    "        \n",
    "    return database\n",
    "\n",
    "def get_pose_image(img, image_size):\n",
    "    landmarks = extract_face_landmarks(img)\n",
    "\n",
    "    if np.all(landmarks == 0) or landmarks is None:\n",
    "        return None\n",
    "\n",
    "    landmarks[:, 0][landmarks[:, 0] >= image_size] = image_size - 1\n",
    "    landmarks[:, 0][landmarks[:, 0] < 0] = 0\n",
    "    landmarks[:, 1][landmarks[:, 1] >= image_size] = image_size - 1\n",
    "    landmarks[:, 1][landmarks[:, 1] < 0] = 0\n",
    "\n",
    "    # landmarks = landmarks[[30, 40, 46, 48, 56]]\n",
    "    # print(landmarks.shape)\n",
    "\n",
    "    pose_img = np.zeros((image_size, image_size))\n",
    "    pose_img[landmarks[:, 1], landmarks[:, 0]] = 1\n",
    "    pose_img = pose_img[:, :, np.newaxis]\n",
    "    pose_img = pose_img[landmarks[:, 1].min():landmarks[:, 1].max(), landmarks[:, 0].min():landmarks[:, 0].max()]\n",
    "    pose_img = cv2.resize(pose_img, (image_size, image_size))\n",
    "    pose_img = np.where(pose_img > 0.5, 1, 0)\n",
    "\n",
    "    # plt.imshow(pose_img)\n",
    "    # plt.show()\n",
    "\n",
    "    pose_img = pose_img[:, :, np.newaxis]\n",
    "\n",
    "    return pose_img\n",
    "\n",
    "def get_magface_embedding(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    if img.shape[:2] != [112, 112]:\n",
    "        img = cv2.resize(img, (112, 112))\n",
    "\n",
    "    img = img[np.newaxis] / 255.\n",
    "    img = torch.Tensor(img).to(\"cuda\").to(torch.float32)\n",
    "    img = img.permute(0, 3, 1, 2)\n",
    "    embedding = magface(img)\n",
    "    embedding = embedding.detach().cpu().numpy()\n",
    "    embedding = np.squeeze(embedding)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def prepare_final_database(database):\n",
    "    db_keys = list(database.keys())\n",
    "\n",
    "    final_database = []\n",
    "    for person1_name in db_keys:\n",
    "        person1 = database[person1_name]\n",
    "        for triple_to_reconstruct_index, triple_to_reconstruct in enumerate(person1):\n",
    "            pose_img_to_reconstruct = triple_to_reconstruct[2]\n",
    "\n",
    "            available_poses_input = []\n",
    "            available_poses_output = []\n",
    "            for person2_name in db_keys:\n",
    "                person2 = database[person2_name]\n",
    "                available_poses_input = list(filter(lambda index: person2[index][0] == triple_to_reconstruct[0], range(len(person2))))\n",
    "                available_poses_output = list(filter(lambda index: person2[index][0] == triple_to_reconstruct[0] and person2[index][1] == triple_to_reconstruct[1], range(len(person2))))\n",
    "\n",
    "                for triple_to_change_input_index in available_poses_input:\n",
    "                    for triple_to_change_output_index in available_poses_output:\n",
    "                        final_database.append((person1_name, triple_to_reconstruct_index, person2_name, triple_to_change_input_index, triple_to_change_output_index))\n",
    "\n",
    "                if len(final_database) >= 60000:\n",
    "                    break\n",
    "\n",
    "            if len(final_database) >= 60000:\n",
    "                break\n",
    "\n",
    "        if len(final_database) >= 60000:\n",
    "            break\n",
    "            \n",
    "    return final_database\n",
    "\n",
    "def save_files(database, final_database, type_):\n",
    "    for idx in tqdm.tqdm(range(len(final_database))):\n",
    "        tup = final_database[idx]\n",
    "\n",
    "        if os.path.isfile(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_pose_img_to_reconstruct.npy') and os.path.isfile(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_input.npy') and os.path.isfile(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_output.npy'):\n",
    "            continue\n",
    "\n",
    "        pose_img_to_reconstruct = database[tup[0]][tup[1]][2]\n",
    "        image_path = database[tup[2]][tup[3]][3]\n",
    "        embedding_input = get_magface_embedding(image_path)\n",
    "        image_path = database[tup[2]][tup[4]][3]\n",
    "        embedding_output = get_magface_embedding(image_path)\n",
    "\n",
    "        with open(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_pose_img_to_reconstruct.npy', 'wb') as f:\n",
    "            np.save(f, pose_img_to_reconstruct)\n",
    "\n",
    "        with open(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_input.npy', 'wb') as f:\n",
    "            np.save(f, embedding_input)\n",
    "\n",
    "        with open(f'/media/soroushh/Storage2/matrices/{type_}/{tup[0]}_{tup[1]}_{tup[2]}_{tup[3]}_{tup[4]}_embedding_output.npy', 'wb') as f:\n",
    "            np.save(f, embedding_output)\n",
    "\n",
    "    with open(f'/media/soroushh/Storage2/database_{type_}.pickle', 'wb') as handle:\n",
    "        pickle.dump(database, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "people_paths = glob.glob(os.path.join(\"./CROPPED_ALIGNED/*\"))\n",
    "people_paths = np.array(people_paths)\n",
    "np.random.shuffle(people_paths)\n",
    "people_paths = people_paths.tolist()\n",
    "\n",
    "index = int(0.8 * len(people_paths))\n",
    "trainset = people_paths[:index]\n",
    "evalset = people_paths[index:]\n",
    "\n",
    "image_paths = [img_path for person_path in trainset for img_path in glob.glob(person_path + \"/*.JPG\")]\n",
    "database = get_landmarks(image_paths, 112)\n",
    "final_database = prepare_final_database(database)\n",
    "save_files(database, final_database, \"training\")\n",
    "\n",
    "image_paths = [img_path for person_path in evalset for img_path in glob.glob(person_path + \"/*.JPG\")]\n",
    "database = get_landmarks(image_paths, 112)\n",
    "final_database = prepare_final_database(database)\n",
    "save_files(database, final_database, \"evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mgLxc6ioKYRK"
   ],
   "name": "MeaningfulEmbeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
